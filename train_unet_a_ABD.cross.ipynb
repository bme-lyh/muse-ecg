{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, LambdaLR\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from network.model_unet_a_2d import *\n",
    "from loss_utils import *\n",
    "from data_loader import *\n",
    "from data_augmentation import *\n",
    "from test_utils import model_predict, dataset_eval\n",
    "from torchinfo import summary\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data.sampler import Sampler\n",
    "import itertools\n",
    "\n",
    "\n",
    "batch_size = 24\n",
    "labeled_bs = 12\n",
    "top_num = 4\n",
    "patch_size_h = 1\n",
    "patch_size_w = 250\n",
    "h_size = 1\n",
    "w_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "def ABD_R(outputs1_max, outputs2_max, volume_batch, volume_batch_strong, outputs1_unlabel, outputs2_unlabel, args):\n",
    "    # ABD-R Bidirectional Displacement Patch\n",
    "    patches_1 = rearrange(outputs1_max[labeled_bs:], 'b (h p1) (w p2)->b (h w) (p1 p2)', p1=patch_size_h, p2=patch_size_w)\n",
    "    patches_2 = rearrange(outputs2_max[labeled_bs:], 'b (h p1) (w p2)->b (h w) (p1 p2)', p1=patch_size_h, p2=patch_size_w)\n",
    "    ecg_patch_1 = rearrange(volume_batch.squeeze(1)[labeled_bs:], 'b  (h p1) (w p2) -> b (h w)(p1 p2) ', p1=patch_size_h, p2=patch_size_w)  # torch.Size([8, 224, 224])\n",
    "    ecg_patch_2 = rearrange(volume_batch_strong.squeeze(1)[labeled_bs:], 'b  (h p1) (w p2) -> b (h w)(p1 p2) ', p1=patch_size_h, p2=patch_size_w)\n",
    "    patches_mean_1 = torch.mean(patches_1.detach(), dim=2)  # torch.Size([8, 16])\n",
    "    patches_mean_2 = torch.mean(patches_2.detach(), dim=2)\n",
    "\n",
    "    patches_outputs_1 = rearrange(outputs1_unlabel, 'b c (h p1) (w p2)->b c (h w) (p1 p2)', p1=patch_size_h, p2=patch_size_w)\n",
    "    patches_outputs_2 = rearrange(outputs2_unlabel, 'b c (h p1) (w p2)->b c (h w) (p1 p2)', p1=patch_size_h, p2=patch_size_w)\n",
    "    patches_mean_outputs_1 = torch.mean(patches_outputs_1.detach(), dim=3).permute(0, 2, 1)  # torch.Size([8, 16, 4])\n",
    "    patches_mean_outputs_2 = torch.mean(patches_outputs_2.detach(), dim=3).permute(0, 2, 1)  # torch.Size([8, 16, 4])\n",
    "\n",
    "    patches_mean_1_top4_values, patches_mean_1_top4_indices = patches_mean_1.topk(top_num, dim=1)  # torch.Size([8, 4])\n",
    "    patches_mean_2_top4_values, patches_mean_2_top4_indices = patches_mean_2.topk(top_num, dim=1)  # torch.Size([8, 4])\n",
    "    for i in range(labeled_bs):\n",
    "        kl_similarities_1 = torch.empty(top_num)\n",
    "        kl_similarities_2 = torch.empty(top_num)\n",
    "        b = torch.argmin(patches_mean_1[i].detach(), dim=0)\n",
    "        d = torch.argmin(patches_mean_2[i].detach(), dim=0)\n",
    "        patches_mean_outputs_min_1 = patches_mean_outputs_1[i, b, :]  # torch.Size([4])\n",
    "        patches_mean_outputs_min_2 = patches_mean_outputs_2[i, d, :]  # torch.Size([4])\n",
    "        patches_mean_outputs_top4_1 = patches_mean_outputs_1[i, patches_mean_1_top4_indices[i, :], :]  # torch.Size([4, 4])\n",
    "        patches_mean_outputs_top4_2 = patches_mean_outputs_2[i, patches_mean_2_top4_indices[i, :], :]  # torch.Size([4, 4])\n",
    "\n",
    "        for j in range(top_num):\n",
    "            kl_similarities_1[j] = torch.nn.functional.kl_div(patches_mean_outputs_top4_1[j].softmax(dim=-1).log(), patches_mean_outputs_min_2.softmax(dim=-1), reduction='sum')\n",
    "            kl_similarities_2[j] = torch.nn.functional.kl_div(patches_mean_outputs_top4_2[j].softmax(dim=-1).log(), patches_mean_outputs_min_1.softmax(dim=-1), reduction='sum')\n",
    "\n",
    "        a = torch.argmin(kl_similarities_1.detach(), dim=0, keepdim=False)\n",
    "        c = torch.argmin(kl_similarities_2.detach(), dim=0, keepdim=False)\n",
    "        a_ori = patches_mean_1_top4_indices[i, a]\n",
    "        c_ori = patches_mean_2_top4_indices[i, c]\n",
    "\n",
    "        max_patch_1 = ecg_patch_2[i][c_ori]  \n",
    "        ecg_patch_1[i][b] = max_patch_1  \n",
    "        max_patch_2 = ecg_patch_1[i][a_ori]\n",
    "        ecg_patch_2[i][d] = max_patch_2 \n",
    "\n",
    "    ecg_patch = torch.cat([ecg_patch_1, ecg_patch_2], dim=0)\n",
    "    ecg_patch_last = rearrange(ecg_patch, 'b (h w)(p1 p2) -> b  (h p1) (w p2)', h=h_size, w=w_size,p1=patch_size_h, p2=patch_size_w) \n",
    "    return ecg_patch_last\n",
    "\n",
    "def ABD_R_BCP(out_max_1, out_max_2, net_input_1, net_input_2, out_1, out_2):\n",
    "    patches_1 = rearrange(out_max_1, 'b (h p1) (w p2)->b (h w) (p1 p2)', p1=patch_size_h, p2=patch_size_w)\n",
    "    patches_2 = rearrange(out_max_2, 'b (h p1) (w p2)->b (h w) (p1 p2)', p1=patch_size_h, p2=patch_size_w)\n",
    "    ecg_patch_1 = rearrange(net_input_1.squeeze(1), 'b  (h p1) (w p2) -> b (h w)(p1 p2) ',p1=patch_size_h, p2=patch_size_w)  # torch.Size([12, 224, 224])\n",
    "    ecg_patch_2 = rearrange(net_input_2.squeeze(1),'b  (h p1) (w p2) -> b (h w)(p1 p2) ', p1=patch_size_h, p2=patch_size_w)\n",
    "\n",
    "    patches_mean_1 = torch.mean(patches_1.detach(), dim=2)\n",
    "    patches_mean_2 = torch.mean(patches_2.detach(), dim=2)\n",
    "\n",
    "    patches_outputs_1 = rearrange(out_1, 'b c (h p1) (w p2)->b c (h w) (p1 p2)', p1=patch_size_h, p2=patch_size_w)\n",
    "    patches_outputs_2 = rearrange(out_2, 'b c (h p1) (w p2)->b c (h w) (p1 p2)', p1=patch_size_h, p2=patch_size_w)\n",
    "    patches_mean_outputs_1 = torch.mean(patches_outputs_1.detach(), dim=3).permute(0, 2, 1)  # torch.Size([8, 16, 4])\n",
    "    patches_mean_outputs_2 = torch.mean(patches_outputs_2.detach(), dim=3).permute(0, 2, 1)  # torch.Size([8, 16, 4])\n",
    "\n",
    "    patches_mean_1_top4_values, patches_mean_1_top4_indices = patches_mean_1.topk(top_num, dim=1)  # torch.Size([8, 4])\n",
    "    patches_mean_2_top4_values, patches_mean_2_top4_indices = patches_mean_2.topk(top_num, dim=1)  # torch.Size([8, 4])\n",
    "\n",
    "    for i in range(labeled_bs):\n",
    "        if random.random() < 0.5:\n",
    "            kl_similarities_1 = torch.empty(top_num)\n",
    "            kl_similarities_2 = torch.empty(top_num)\n",
    "            b = torch.argmin(patches_mean_1[i].detach(), dim=0)\n",
    "            d = torch.argmin(patches_mean_2[i].detach(), dim=0)\n",
    "            patches_mean_outputs_min_1 = patches_mean_outputs_1[i, b, :]  # torch.Size([4])\n",
    "            patches_mean_outputs_min_2 = patches_mean_outputs_2[i, d, :]  # torch.Size([4])\n",
    "\n",
    "            patches_mean_outputs_top4_1 = patches_mean_outputs_1[i, patches_mean_1_top4_indices[i, :], :]  # torch.Size([4, 4])\n",
    "            patches_mean_outputs_top4_2 = patches_mean_outputs_2[i, patches_mean_2_top4_indices[i, :], :]  # torch.Size([4, 4])\n",
    "\n",
    "            for j in range(top_num):\n",
    "                kl_similarities_1[j] = torch.nn.functional.kl_div(patches_mean_outputs_top4_1[j].softmax(dim=-1).log(), patches_mean_outputs_min_2.softmax(dim=-1), reduction='sum')\n",
    "                kl_similarities_2[j] = torch.nn.functional.kl_div(patches_mean_outputs_top4_2[j].softmax(dim=-1).log(), patches_mean_outputs_min_1.softmax(dim=-1), reduction='sum')\n",
    "\n",
    "            a = torch.argmin(kl_similarities_1.detach(), dim=0, keepdim=False)\n",
    "            c = torch.argmin(kl_similarities_2.detach(), dim=0, keepdim=False)\n",
    "\n",
    "            a_ori = patches_mean_1_top4_indices[i, a]\n",
    "            c_ori = patches_mean_2_top4_indices[i, c]\n",
    "\n",
    "            max_patch_1 = ecg_patch_2[i][c_ori]  \n",
    "            ecg_patch_1[i][b] = max_patch_1  \n",
    "            max_patch_2 = ecg_patch_1[i][a_ori]\n",
    "            ecg_patch_2[i][d] = max_patch_2\n",
    "        else:\n",
    "            a = torch.argmax(patches_mean_1[i].detach(), dim=0)\n",
    "            b = torch.argmin(patches_mean_1[i].detach(), dim=0)\n",
    "            c = torch.argmax(patches_mean_2[i].detach(), dim=0)\n",
    "            d = torch.argmin(patches_mean_2[i].detach(), dim=0)\n",
    "\n",
    "            max_patch_1 = ecg_patch_2[i][c]  \n",
    "            ecg_patch_1[i][b] = max_patch_1  \n",
    "            max_patch_2 = ecg_patch_1[i][a]\n",
    "            ecg_patch_2[i][d] = max_patch_2\n",
    "    ecg_patch = torch.cat([ecg_patch_1, ecg_patch_2], dim=0)\n",
    "    ecg_patch_last = rearrange(ecg_patch, 'b (h w)(p1 p2) -> b  (h p1) (w p2)', h=h_size, w=w_size,p1=patch_size_h, p2=patch_size_w)\n",
    "    return ecg_patch_last\n",
    "\n",
    "\n",
    "def ABD_I(outputs1_max, outputs2_max, volume_batch, volume_batch_strong, label_batch, label_batch_strong, args):\n",
    "    # ABD-I Bidirectional Displacement Patch\n",
    "    patches_supervised_1 = rearrange(outputs1_max[:labeled_bs], 'b (h p1) (w p2)->b (h w) (p1 p2)', p1=patch_size_h, p2=patch_size_w)\n",
    "    patches_supervised_2 = rearrange(outputs2_max[:labeled_bs], 'b (h p1) (w p2)->b (h w) (p1 p2)', p1=patch_size_h, p2=patch_size_w)\n",
    "    ecg_patch_supervised_1 = rearrange(volume_batch.squeeze(1)[:labeled_bs], 'b  (h p1) (w p2) -> b (h w)(p1 p2) ', p1=patch_size_h, p2=patch_size_w)  # torch.Size([8, 224, 224])\n",
    "    ecg_patch_supervised_2 = rearrange(volume_batch_strong.squeeze(1)[:labeled_bs], 'b  (h p1) (w p2) -> b (h w)(p1 p2) ', p1=patch_size_h, p2=patch_size_w)\n",
    "    label_patch_supervised_1 = rearrange(label_batch[:labeled_bs], 'b  (h p1) (w p2) -> b (h w)(p1 p2) ', p1=patch_size_h, p2=patch_size_w)\n",
    "    label_patch_supervised_2 = rearrange(label_batch_strong[:labeled_bs], 'b  (h p1) (w p2) -> b (h w)(p1 p2) ', p1=patch_size_h, p2=patch_size_w)\n",
    "    patches_mean_supervised_1 = torch.mean(patches_supervised_1.detach(), dim=2)\n",
    "    patches_mean_supervised_2 = torch.mean(patches_supervised_2.detach(), dim=2)\n",
    "    e = torch.argmax(patches_mean_supervised_1.detach(), dim=1)\n",
    "    f = torch.argmin(patches_mean_supervised_1.detach(), dim=1)\n",
    "    g = torch.argmax(patches_mean_supervised_2.detach(), dim=1)\n",
    "    h = torch.argmin(patches_mean_supervised_2.detach(), dim=1)\n",
    "    for i in range(labeled_bs): \n",
    "        if random.random() < 0.5:\n",
    "            min_patch_supervised_1 = ecg_patch_supervised_2[i][h[i]]  \n",
    "            ecg_patch_supervised_1[i][e[i]] = min_patch_supervised_1\n",
    "            min_patch_supervised_2 = ecg_patch_supervised_1[i][f[i]]\n",
    "            ecg_patch_supervised_2[i][g[i]] = min_patch_supervised_2\n",
    "\n",
    "            min_label_supervised_1 = label_patch_supervised_2[i][h[i]]\n",
    "            label_patch_supervised_1[i][e[i]] = min_label_supervised_1\n",
    "            min_label_supervised_2 = label_patch_supervised_1[i][f[i]]\n",
    "            label_patch_supervised_2[i][g[i]] = min_label_supervised_2\n",
    "    ecg_patch_supervised = torch.cat([ecg_patch_supervised_1, ecg_patch_supervised_2], dim=0)\n",
    "    ecg_patch_supervised_last = rearrange(ecg_patch_supervised, 'b (h w)(p1 p2) -> b  (h p1) (w p2)', h=h_size, w=w_size,p1=patch_size_h, p2=patch_size_w)  # torch.Size([16, 224, 224])\n",
    "    label_patch_supervised = torch.cat([label_patch_supervised_1, label_patch_supervised_2], dim=0)\n",
    "    label_patch_supervised_last = rearrange(label_patch_supervised, 'b (h w)(p1 p2) -> b  (h p1) (w p2)', h=h_size, w=w_size,p1=patch_size_h, p2=patch_size_w)  # torch.Size([16, 224, 224])\n",
    "    return ecg_patch_supervised_last, label_patch_supervised_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "def iterate_once(iterable):\n",
    "    return np.random.permutation(iterable)\n",
    "\n",
    "\n",
    "def iterate_eternally(indices):\n",
    "    def infinite_shuffles():\n",
    "        while True:\n",
    "            yield np.random.permutation(indices)\n",
    "    return itertools.chain.from_iterable(infinite_shuffles())\n",
    "\n",
    "\n",
    "def grouper(iterable, n):\n",
    "    \"Collect data into fixed-length chunks or blocks\"\n",
    "    # grouper('ABCDEFG', 3) --> ABC DEF\"\n",
    "    args = [iter(iterable)] * n\n",
    "    return zip(*args)\n",
    "\n",
    "\n",
    "class TwoStreamBatchSampler(Sampler):\n",
    "    \"\"\"Iterate two sets of indices\n",
    "\n",
    "    An 'epoch' is one iteration through the primary indices.\n",
    "    During the epoch, the secondary indices are iterated through\n",
    "    as many times as needed.\n",
    "    \"\"\"\n",
    "    def __init__(self, primary_indices, secondary_indices, batch_size, secondary_batch_size):\n",
    "        self.primary_indices = primary_indices\n",
    "        self.secondary_indices = secondary_indices\n",
    "        self.secondary_batch_size = secondary_batch_size\n",
    "        self.primary_batch_size = batch_size - secondary_batch_size\n",
    "\n",
    "        assert len(self.primary_indices) >= self.primary_batch_size > 0\n",
    "        assert len(self.secondary_indices) >= self.secondary_batch_size > 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        primary_iter = iterate_once(self.primary_indices)\n",
    "        secondary_iter = iterate_eternally(self.secondary_indices)\n",
    "        return (\n",
    "            primary_batch + secondary_batch\n",
    "            for (primary_batch, secondary_batch)\n",
    "            in zip(grouper(primary_iter, self.primary_batch_size),\n",
    "                    grouper(secondary_iter, self.secondary_batch_size))\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.primary_indices) // self.primary_batch_size\n",
    "\n",
    "\n",
    "def cutout_ecg(ecg, label, p=0.5, size_min=0.02, size_max=0.4, value_min=0, value_max=1, pixel_level=True):\n",
    "    \"\"\"\n",
    "    应用Cutout增强到ECG信号及其对应的标签。\n",
    "    参数：\n",
    "        ecg：ECG信号，形状为 (length, channels=1)\n",
    "        label：标签，形状为 (length, channels=4)\n",
    "        p：应用Cutout的概率\n",
    "        size_min, size_max：擦除区域长度的最小和最大比例\n",
    "        value_min, value_max：ECG填充值的范围\n",
    "        pixel_level：是否使用逐位置随机值进行填充\n",
    "    返回：\n",
    "        ecg：增强后的ECG信号\n",
    "        label：增强后的标签\n",
    "    \"\"\"\n",
    "    if random.random() < p:\n",
    "        # 确保输入为NumPy数组\n",
    "        ecg = np.array(ecg)\n",
    "        label = np.array(label)\n",
    "\n",
    "        # 检查输入形状\n",
    "        assert ecg.ndim == 2 and label.ndim == 2, \"输入必须为二维数组\"\n",
    "        assert ecg.shape[1] == 1 and label.shape[1] == 4, \"ECG的通道数必须为1，标签的通道数必须为4\"\n",
    "        assert ecg.shape[0] == label.shape[0], \"ECG和标签的长度必须匹配\"\n",
    "        \n",
    "        # 获取长度 (L)\n",
    "        L = ecg.shape[0]\n",
    "\n",
    "        # 确定擦除区域的宽度\n",
    "        size = np.random.uniform(size_min, size_max) * L\n",
    "        erase_w = max(1, int(size))  # 确保擦除宽度至少为1\n",
    "\n",
    "        # 选择起始位置\n",
    "        x = np.random.randint(0, L - erase_w + 1)\n",
    "\n",
    "        # 为ECG生成填充值\n",
    "        if pixel_level:\n",
    "            value = np.random.randint(value_min, value_max + 1, (erase_w, 1))\n",
    "        else:\n",
    "            value = np.random.randint(value_min, value_max + 1)\n",
    "\n",
    "        # 对ECG应用Cutout\n",
    "        ecg[x:x + erase_w, :] = value\n",
    "\n",
    "        # 对标签应用Cutout，设置为0\n",
    "        label[x:x + erase_w, :] = 0\n",
    "\n",
    "    return ecg, label\n",
    "\n",
    "\n",
    "class WeakStrongAugment(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, ecg, label):\n",
    "        ecg, label = random_resize(ecg, label, scale_range=(0.5, 2))\n",
    "        if random.random() < 0.5:\n",
    "            ecg, label = np.flip(ecg, axis=0).copy(), np.flip(label, axis=0).copy()\n",
    "\n",
    "        # strong augmentation is color jitter\n",
    "        ecg_strong, label_strong = cutout_ecg(ecg,label,p=0.5)\n",
    "        if random.random() < 0.5:\n",
    "            ecg_strong = ecg_strong + baseline_wander_noise(ecg_strong[:,0], fs=500, snr=-10, freq=0.15)[:,np.newaxis]\n",
    "        if random.random() < 0.5:\n",
    "            ecg_strong = ecg_strong + additive_white_gaussian_noise(ecg_strong[:,0], snr=10)[:,np.newaxis]\n",
    "\n",
    "        ecg = zscore_normalize(ecg, axis=0)\n",
    "        ecg_strong = zscore_normalize(ecg_strong, axis=0)\n",
    "\n",
    "        ecg = torch.from_numpy(ecg.astype(np.float32)).permute(1, 0).unsqueeze(1) # (channel=1, 1, length)\n",
    "        ecg_strong = torch.from_numpy(ecg_strong.astype(np.float32)).permute(1, 0).unsqueeze(1)\n",
    "        label = torch.from_numpy(label.astype(np.float32)).permute(1, 0).unsqueeze(1)\n",
    "        label_strong = torch.from_numpy(label_strong.astype(np.float32)).permute(1, 0).unsqueeze(1)\n",
    "\n",
    "        sample = {\n",
    "            \"ecg\": ecg,\n",
    "            \"ecg_strong\": ecg_strong,\n",
    "            \"label\": label,\n",
    "            \"label_strong\": label_strong\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "\n",
    "class ECGDataset_ABD(Dataset):\n",
    "    def __init__(self, x, y, transform=None, weak_strong_augment=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "        self.weak_strong_augment = weak_strong_augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_sample = self.x[idx]\n",
    "        y_sample = self.y[idx]\n",
    "        if self.weak_strong_augment:\n",
    "            sample = self.weak_strong_augment(x_sample, y_sample)\n",
    "            return sample\n",
    "        \n",
    "        if self.transform:\n",
    "            for transform_func in self.transform:\n",
    "                x_sample, y_sample = transform_func(x_sample, y_sample)\n",
    "\n",
    "        x_sample = torch.from_numpy(x_sample.astype(np.float32)).permute(1, 0).unsqueeze(1) # (channel=1, 1, length)\n",
    "        y_sample = torch.from_numpy(y_sample.astype(np.float32)).permute(1, 0).unsqueeze(1) # (channel=4, 1, length)\n",
    "        sample = {\n",
    "            \"ecg\": x_sample,\n",
    "            \"label\": y_sample\n",
    "        }\n",
    "        return sample\n",
    "    \n",
    "\n",
    "def generate_mask(img):\n",
    "    \"\"\"\n",
    "    随机生成遮罩，返回遮罩及对应的损失遮罩。\n",
    "    针对 H=1 的数据，确保 patch_x 至少为1，并处理随机起始位置的边界条件。\n",
    "    \"\"\"\n",
    "    batch_size, channel, img_x, img_y = img.shape\n",
    "    loss_mask = torch.ones(batch_size, img_x, img_y).cuda()\n",
    "    mask = torch.ones(img_x, img_y).cuda()\n",
    "    # 如果 img_x == 1，则 force patch_x 为1；否则按比例计算\n",
    "    patch_x = int(img_x * 2 / 3) if img_x > 1 else 1\n",
    "    patch_y = int(img_y * 2 / 3)\n",
    "    # 计算随机起始位置时，防止负值\n",
    "    if img_x - patch_x <= 0:\n",
    "        w = 0\n",
    "    else:\n",
    "        w = np.random.randint(0, img_x - patch_x)\n",
    "    if img_y - patch_y <= 0:\n",
    "        h = 0\n",
    "    else:\n",
    "        h = np.random.randint(0, img_y - patch_y)\n",
    "    mask[w:w + patch_x, h:h + patch_y] = 0\n",
    "    loss_mask[:, w:w + patch_x, h:h + patch_y] = 0\n",
    "    return mask.long(), loss_mask.long()\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def _one_hot_encoder(self, input_tensor):\n",
    "        tensor_list = []\n",
    "        for i in range(self.n_classes):\n",
    "            temp_prob = input_tensor == i * torch.ones_like(input_tensor)\n",
    "            tensor_list.append(temp_prob)\n",
    "        output_tensor = torch.cat(tensor_list, dim=1)\n",
    "        return output_tensor.float()\n",
    "\n",
    "    def _dice_loss(self, score, target):\n",
    "        target = target.float()\n",
    "        smooth = 1e-10\n",
    "        intersect = torch.sum(score * target)\n",
    "        y_sum = torch.sum(target * target)\n",
    "        z_sum = torch.sum(score * score)\n",
    "        loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth)\n",
    "        loss = 1 - loss\n",
    "        return loss\n",
    "    \n",
    "    def _dice_mask_loss(self, score, target, mask):\n",
    "        target = target.float()\n",
    "        mask = mask.float()\n",
    "        smooth = 1e-10\n",
    "        intersect = torch.sum(score * target * mask)\n",
    "        y_sum = torch.sum(target * target * mask)\n",
    "        z_sum = torch.sum(score * score * mask)\n",
    "        loss = (2 * intersect + smooth ) / (z_sum + y_sum + smooth)\n",
    "        loss = 1 - loss\n",
    "        return loss\n",
    "\n",
    "    def forward(self, inputs, target, mask=None, weight=None, softmax=True, one_hot=False):\n",
    "        # If softmax is True, apply softmax to the inputs\n",
    "        if softmax:\n",
    "            inputs = torch.softmax(inputs, dim=1)\n",
    "\n",
    "        # One-hot encoding of target, only if one_hot is True\n",
    "        if one_hot:\n",
    "            target = self._one_hot_encoder(target)\n",
    "        \n",
    "        if weight is None:\n",
    "            weight = [1] * self.n_classes\n",
    "\n",
    "        assert inputs.size() == target.size(), 'predict & target shape do not match'\n",
    "        \n",
    "        class_wise_dice = []\n",
    "        loss = 0.0\n",
    "        if mask is not None:\n",
    "            # Expand mask to match the number of classes\n",
    "            mask = mask.repeat(1, self.n_classes, 1, 1).type(torch.float32)\n",
    "            for i in range(0, self.n_classes): \n",
    "                dice = self._dice_mask_loss(inputs[:, i], target[:, i], mask[:, i])\n",
    "                class_wise_dice.append(1.0 - dice.item())\n",
    "                loss += dice * weight[i]\n",
    "        else:\n",
    "            for i in range(0, self.n_classes):\n",
    "                dice = self._dice_loss(inputs[:, i], target[:, i])\n",
    "                class_wise_dice.append(1.0 - dice.item())\n",
    "                loss += dice * weight[i]\n",
    "        \n",
    "        return loss / self.n_classes\n",
    "    \n",
    "\n",
    "def mix_loss(output, img_l, patch_l, mask, l_weight=1.0, u_weight=0.5, unlab=False):\n",
    "    CE = nn.CrossEntropyLoss(reduction='none')\n",
    "    dice_loss = DiceLoss(n_classes=4)\n",
    "    img_l, patch_l = img_l.type(torch.int64), patch_l.type(torch.int64)\n",
    "    output_soft = F.softmax(output, dim=1)\n",
    "    ecg_weight, patch_weight = l_weight, u_weight\n",
    "    if unlab:\n",
    "        ecg_weight, patch_weight = u_weight, l_weight\n",
    "    patch_mask = 1 - mask\n",
    "    loss_dice = dice_loss(output_soft, img_l, mask.unsqueeze(1)) * ecg_weight\n",
    "    loss_dice += dice_loss(output_soft, patch_l, patch_mask.unsqueeze(1)) * patch_weight\n",
    "    # loss_ce = ecg_weight * (CE(output, img_l) * mask).sum() / (mask.sum() + 1e-16) \n",
    "    # loss_ce += patch_weight * (CE(output, patch_l) * patch_mask).sum() / (patch_mask.sum() + 1e-16)#loss = loss_ce\n",
    "    loss_ce = ecg_weight * (CE(output, img_l.argmax(dim=1)) * mask).sum() / (mask.sum() + 1e-16) \n",
    "    loss_ce += patch_weight * (CE(output, patch_l.argmax(dim=1)) * patch_mask).sum() / (patch_mask.sum() + 1e-16)#loss = loss_ce\n",
    "    return loss_dice, loss_ce\n",
    "\n",
    "\n",
    "def pre_train(model, snapshot_path, db_train_l, db_train_u, db_val):\n",
    "    base_lr = 0.01\n",
    "    num_classes = 4\n",
    "    # max_iterations = 10000\n",
    "    max_iterations = 1000\n",
    "    # max_iterations = 1\n",
    "\n",
    "    batch_size = 24\n",
    "    labeled_bs = 12\n",
    "    seed = 42\n",
    "    labeled_sub_bs, unlabeled_sub_bs = int(labeled_bs/2), int((batch_size-labeled_bs) / 2)\n",
    "\n",
    "    def worker_init_fn(worker_id):\n",
    "        random.seed(seed + worker_id)\n",
    "\n",
    "    db_train = torch.utils.data.ConcatDataset([db_train_l, db_train_u])\n",
    "    num_total = len(db_train)\n",
    "    num_labeled = len(db_train_l)\n",
    "    labeled_idxs = list(range(0, num_labeled))\n",
    "    unlabeled_idxs = list(range(num_labeled, num_total))\n",
    "    batch_sampler = TwoStreamBatchSampler(labeled_idxs, unlabeled_idxs, batch_size, batch_size-labeled_bs)\n",
    "\n",
    "    trainloader = DataLoader(db_train, batch_sampler=batch_sampler, num_workers=4, pin_memory=True, worker_init_fn=worker_init_fn)\n",
    "\n",
    "    valloader = DataLoader(db_val, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=base_lr, momentum=0.9, weight_decay=0.0001)\n",
    "\n",
    "    model.train()\n",
    "    iter_num = 0\n",
    "    max_epoch = max_iterations // len(trainloader) + 1\n",
    "    best_val_loss = np.inf\n",
    "    iterator = tqdm(range(max_epoch), ncols=70)\n",
    "    for _ in iterator:\n",
    "        for _, sampled_batch in enumerate(trainloader):\n",
    "            volume_batch, label_batch = sampled_batch['ecg'], sampled_batch['label']\n",
    "            volume_batch, label_batch = volume_batch.cuda(), label_batch.cuda()\n",
    "\n",
    "            img_a, img_b = volume_batch[:labeled_sub_bs], volume_batch[labeled_sub_bs:labeled_bs]\n",
    "            lab_a, lab_b = label_batch[:labeled_sub_bs], label_batch[labeled_sub_bs:labeled_bs]\n",
    "            img_mask, loss_mask = generate_mask(img_a)\n",
    "            gt_mixl = lab_a * img_mask + lab_b * (1 - img_mask)\n",
    "\n",
    "            #-- original\n",
    "            net_input = img_a * img_mask + img_b * (1 - img_mask)\n",
    "            out_mixl = model(net_input)\n",
    "            loss_dice, loss_ce = mix_loss(out_mixl, lab_a, lab_b, loss_mask, u_weight=1.0, unlab=True)\n",
    "\n",
    "            loss = (loss_dice + loss_ce) / 2            \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            iter_num += 1\n",
    "\n",
    "            if iter_num > 0 and iter_num % 100 == 0:\n",
    "            # if 1:\n",
    "                model.eval()\n",
    "                val_loss = 0\n",
    "                for _, sampled_batch in enumerate(valloader):\n",
    "                    val_volume, val_label = sampled_batch['ecg'], sampled_batch['label']\n",
    "                    val_volume, val_label = val_volume.cuda(), val_label.cuda()\n",
    "                    val_output = model(val_volume)\n",
    "                    val_loss += DiceLoss(n_classes=4)(val_output, val_label).item()\n",
    "                val_loss /= len(valloader)\n",
    "                iterator.set_postfix({'Iter': iter_num, 'Val Loss': f'{val_loss:.4f}'})\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    torch.save(model.state_dict(), snapshot_path)\n",
    "\n",
    "                model.train()\n",
    "\n",
    "            if iter_num >= max_iterations:\n",
    "                break\n",
    "        if iter_num >= max_iterations:\n",
    "            iterator.close()\n",
    "            break\n",
    "\n",
    "\n",
    "def get_ACDC_masks(output):\n",
    "    probs = F.softmax(output, dim=1)\n",
    "    _, probs = torch.max(output, dim=1)\n",
    "    probs = F.one_hot(probs.squeeze(1).squeeze(1), num_classes=4).permute(0, 2, 1).unsqueeze(2).float()\n",
    "    # if nms == 1:\n",
    "    #     probs = get_ACDC_2DLargestCC(probs)      \n",
    "    return probs\n",
    "\n",
    "def sigmoid_rampup(current, rampup_length):\n",
    "    \"\"\"Exponential rampup from https://arxiv.org/abs/1610.02242\"\"\"\n",
    "    if rampup_length == 0:\n",
    "        return 1.0\n",
    "    else:               \n",
    "        current = np.clip(current, 0.0, rampup_length)\n",
    "        phase = 1.0 - current / rampup_length\n",
    "        return float(np.exp(-5.0 * phase * phase))\n",
    "    \n",
    "def get_current_consistency_weight(epoch):\n",
    "    # Consistency ramp-up from https://arxiv.org/abs/1610.02242\n",
    "    consistency = 0.1\n",
    "    consistency_rampup = 200.0\n",
    "    return 5* consistency * sigmoid_rampup(epoch, consistency_rampup)\n",
    "\n",
    "def update_model_ema(model, ema_model, alpha):\n",
    "    model_state = model.state_dict()\n",
    "    model_ema_state = ema_model.state_dict()\n",
    "    new_dict = {}\n",
    "    for key in model_state:\n",
    "        new_dict[key] = alpha * model_ema_state[key] + (1 - alpha) * model_state[key]\n",
    "    ema_model.load_state_dict(new_dict)\n",
    "\n",
    "\n",
    "def self_train(model_1, model_2, ema_model, snapshot_path, final_path_1, final_path_2, db_train_l, db_train_u, db_val):\n",
    "    base_lr = 0.01\n",
    "    num_classes = 4\n",
    "    # max_iterations = 30000\n",
    "    max_iterations = 1000\n",
    "    # max_iterations = 1\n",
    "    \n",
    "    seed = 42\n",
    "    u_weight = 0.5\n",
    "    labeled_sub_bs, unlabeled_sub_bs = int(labeled_bs/2), int((batch_size-labeled_bs) / 2)\n",
    "\n",
    "    def worker_init_fn(worker_id):\n",
    "        random.seed(seed + worker_id)\n",
    "\n",
    "    db_train = torch.utils.data.ConcatDataset([db_train_l, db_train_u])\n",
    "    num_total = len(db_train)\n",
    "    num_labeled = len(db_train_l)\n",
    "    labeled_idxs = list(range(0, num_labeled))\n",
    "    unlabeled_idxs = list(range(num_labeled, num_total))\n",
    "    batch_sampler = TwoStreamBatchSampler(labeled_idxs, unlabeled_idxs, batch_size, batch_size-labeled_bs)\n",
    "\n",
    "    trainloader = DataLoader(db_train, batch_sampler=batch_sampler, num_workers=4, pin_memory=True, worker_init_fn=worker_init_fn)\n",
    "\n",
    "    valloader = DataLoader(db_val, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "    optimizer1 = torch.optim.SGD(model.parameters(), lr=base_lr, momentum=0.9, weight_decay=0.0001)\n",
    "    optimizer2 = torch.optim.SGD(model.parameters(), lr=base_lr, momentum=0.9, weight_decay=0.0001)\n",
    "\n",
    "    model_1.load_state_dict(torch.load(snapshot_path))    \n",
    "    model_2.load_state_dict(torch.load(snapshot_path))\n",
    "    ema_model.load_state_dict(torch.load(snapshot_path))\n",
    "\n",
    "    model_1.train()\n",
    "    model_2.train()\n",
    "    ema_model.train()\n",
    "\n",
    "    iter_num = 0\n",
    "    max_epoch = max_iterations // len(trainloader) + 1\n",
    "    best_val_loss_1 = np.inf\n",
    "    best_val_loss_2 = np.inf\n",
    "    iterator = tqdm(range(max_epoch), ncols=70)\n",
    "    for _ in iterator:\n",
    "        for _, sampled_batch in enumerate(trainloader):\n",
    "            volume_batch, label_batch = sampled_batch['ecg'], sampled_batch['label']\n",
    "            volume_batch, label_batch = volume_batch.cuda(), label_batch.cuda()\n",
    "            volume_batch_strong, label_batch_strong = sampled_batch['ecg_strong'], sampled_batch['label_strong']\n",
    "            volume_batch_strong, label_batch_strong = volume_batch_strong.cuda(), label_batch_strong.cuda()\n",
    "\n",
    "            img_a, img_b = volume_batch[:labeled_sub_bs], volume_batch[labeled_sub_bs:labeled_bs]\n",
    "            uimg_a, uimg_b = volume_batch[labeled_bs:labeled_bs + unlabeled_sub_bs], volume_batch[labeled_bs + unlabeled_sub_bs:]\n",
    "            lab_a, lab_b = label_batch[:labeled_sub_bs], label_batch[labeled_sub_bs:labeled_bs]\n",
    "\n",
    "            img_a_s, img_b_s = volume_batch_strong[:labeled_sub_bs], volume_batch_strong[labeled_sub_bs:labeled_bs]\n",
    "            uimg_a_s, uimg_b_s = volume_batch_strong[labeled_bs:labeled_bs + unlabeled_sub_bs], volume_batch_strong[labeled_bs + unlabeled_sub_bs:]\n",
    "            lab_a_s, lab_b_s = label_batch_strong[:labeled_sub_bs], label_batch_strong[labeled_sub_bs:labeled_bs]\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                pre_a = ema_model(uimg_a)\n",
    "                pre_b = ema_model(uimg_b)\n",
    "                plab_a = get_ACDC_masks(pre_a)\n",
    "                plab_b = get_ACDC_masks(pre_b)\n",
    "                pre_a_s = ema_model(uimg_a_s)\n",
    "                pre_b_s = ema_model(uimg_b_s)\n",
    "                plab_a_s = get_ACDC_masks(pre_a_s)\n",
    "                plab_b_s = get_ACDC_masks(pre_b_s)\n",
    "                img_mask, loss_mask = generate_mask(img_a)\n",
    "            consistency_weight = get_current_consistency_weight(iter_num//150)\n",
    "\n",
    "            net_input_unl_1 = uimg_a * img_mask + img_a * (1 - img_mask)\n",
    "            net_input_l_1 = img_b * img_mask + uimg_b * (1 - img_mask)\n",
    "            net_input_1 = torch.cat([net_input_unl_1, net_input_l_1], dim=0) \n",
    "\n",
    "            net_input_unl_2 = uimg_a_s * img_mask + img_a_s * (1 - img_mask)\n",
    "            net_input_l_2 = img_b_s * img_mask + uimg_b_s * (1 - img_mask)\n",
    "            net_input_2 = torch.cat([net_input_unl_2, net_input_l_2], dim=0)\n",
    "\n",
    "\n",
    "            # Model1 Loss\n",
    "            out_unl_1 = model_1(net_input_unl_1)\n",
    "            out_l_1 = model_1(net_input_l_1)\n",
    "            out_1 = torch.cat([out_unl_1, out_l_1], dim=0)\n",
    "            out_soft_1 = torch.softmax(out_1, dim=1)\n",
    "            out_max_1 = torch.max(out_soft_1.detach(), dim=1)[0]\n",
    "            out_pseudo_1 = torch.argmax(out_soft_1.detach(), dim=1, keepdim=False) \n",
    "            unl_dice_1, unl_ce_1 = mix_loss(out_unl_1, plab_a, lab_a, loss_mask, u_weight=u_weight, unlab=True)\n",
    "            l_dice_1, l_ce_1 = mix_loss(out_l_1, lab_b, plab_b, loss_mask, u_weight=u_weight)\n",
    "            loss_ce_1 = unl_ce_1 + l_ce_1\n",
    "            loss_dice_1 = unl_dice_1 + l_dice_1\n",
    "\n",
    "            # Model2 Loss\n",
    "            out_unl_2 = model_2(net_input_unl_2)\n",
    "            out_l_2 = model_2(net_input_l_2)\n",
    "            out_2 = torch.cat([out_unl_2, out_l_2], dim=0)\n",
    "            out_soft_2 = torch.softmax(out_2, dim=1)\n",
    "            out_max_2 = torch.max(out_soft_2.detach(), dim=1)[0]\n",
    "            out_pseudo_2 = torch.argmax(out_soft_2.detach(), dim=1, keepdim=False) \n",
    "            unl_dice_2, unl_ce_2 = mix_loss(out_unl_2, plab_a_s, lab_a_s, loss_mask, u_weight=u_weight, unlab=True)\n",
    "            l_dice_2, l_ce_2 = mix_loss(out_l_2, lab_b_s, plab_b_s, loss_mask, u_weight=u_weight)\n",
    "            loss_ce_2 = unl_ce_2 + l_ce_2\n",
    "            loss_dice_2 = unl_dice_2 + l_dice_2\n",
    "\n",
    "            dice_loss = DiceLoss(n_classes=4)\n",
    "            # Model1 & Model2 Cross Pseudo Supervision\n",
    "            pseudo_supervision1 = dice_loss(out_soft_1, out_pseudo_2.unsqueeze(1), softmax=False, one_hot=True)  \n",
    "            pseudo_supervision2 = dice_loss(out_soft_2, out_pseudo_1.unsqueeze(1), softmax=False, one_hot=True)  \n",
    "            # ABD-R New Training Sample\n",
    "            ecg_patch_last = ABD_R_BCP(out_max_1, out_max_2, net_input_1, net_input_2, out_1, out_2)\n",
    "            ecg_output_1 = model_1(ecg_patch_last.unsqueeze(1))  \n",
    "            ecg_output_soft_1 = torch.softmax(ecg_output_1, dim=1)\n",
    "            pseudo_ecg_output_1 = torch.argmax(ecg_output_soft_1.detach(), dim=1, keepdim=False)\n",
    "            ecg_output_2 = model_2(ecg_patch_last.unsqueeze(1))\n",
    "            ecg_output_soft_2 = torch.softmax(ecg_output_2, dim=1)\n",
    "            pseudo_ecg_output_2 = torch.argmax(ecg_output_soft_2.detach(), dim=1, keepdim=False)\n",
    "            # Model1 & Model2 Second Step Cross Pseudo Supervision\n",
    "            pseudo_supervision3 = dice_loss(ecg_output_soft_1, pseudo_ecg_output_2.unsqueeze(1), softmax=False, one_hot=True)\n",
    "            pseudo_supervision4 = dice_loss(ecg_output_soft_2, pseudo_ecg_output_1.unsqueeze(1), softmax=False, one_hot=True)\n",
    "\n",
    "            loss_1 = (loss_dice_1 + loss_ce_1) / 2 + pseudo_supervision1 + pseudo_supervision3\n",
    "            loss_2 = (loss_dice_2 + loss_ce_2) / 2 + pseudo_supervision2 + pseudo_supervision4\n",
    "            loss = loss_1 + loss_2\n",
    "\n",
    "\n",
    "            optimizer1.zero_grad()\n",
    "            optimizer2.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer1.step()\n",
    "            optimizer2.step()\n",
    "\n",
    "            iter_num += 1\n",
    "            update_model_ema(model_1, ema_model, 0.99)\n",
    "\n",
    "            if iter_num > 0 and iter_num % 100 == 0:\n",
    "            # if 1:\n",
    "                model_1.eval()\n",
    "                val_loss = 0\n",
    "                for _, sampled_batch in enumerate(valloader):\n",
    "                    val_volume, val_label = sampled_batch['ecg'], sampled_batch['label']\n",
    "                    val_volume, val_label = val_volume.cuda(), val_label.cuda()\n",
    "                    val_output = model_1(val_volume)\n",
    "                    val_loss += DiceLoss(n_classes=4)(val_output, val_label).item()\n",
    "                val_loss /= len(valloader)\n",
    "\n",
    "                if val_loss < best_val_loss_1:\n",
    "                    best_val_loss_1 = val_loss\n",
    "                    torch.save(model_1.state_dict(), final_path_1)\n",
    "                model_1.train()\n",
    "\n",
    "                model_2.eval()\n",
    "                val_loss = 0\n",
    "                for _, sampled_batch in enumerate(valloader):\n",
    "                    val_volume, val_label = sampled_batch['ecg'], sampled_batch['label']\n",
    "                    val_volume, val_label = val_volume.cuda(), val_label.cuda()\n",
    "                    val_output = model_2(val_volume)\n",
    "                    val_loss += DiceLoss(n_classes=4)(val_output, val_label).item()\n",
    "                val_loss /= len(valloader)\n",
    "\n",
    "                if val_loss < best_val_loss_2:\n",
    "                    best_val_loss_2 = val_loss\n",
    "                    torch.save(model_2.state_dict(), final_path_2)\n",
    "                model_2.train()\n",
    "\n",
    "                iterator.set_postfix({'Iter': iter_num, 'Val Loss1': f'{best_val_loss_1:.4f}', 'Val Loss2': f'{best_val_loss_2:.4f}'})    \n",
    "\n",
    "            if iter_num >= max_iterations:\n",
    "                break\n",
    "        if iter_num >= max_iterations:\n",
    "            iterator.close()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labeled data: 5\n",
      "Training Stage\n",
      "Test Stage\n",
      "                mean       std       max       min     micro\n",
      "iou_p       0.654467  0.052165  0.718982  0.598013  0.649111\n",
      "iou_qrs     0.828871  0.029985  0.858051  0.785064  0.820951\n",
      "iou_t       0.741357  0.040647  0.786865  0.686746  0.738431\n",
      "miou        0.741565  0.039961  0.783642  0.689941  0.736165\n",
      "acc         0.888106  0.015212  0.902747  0.865644  0.886333\n",
      "ave_f1      0.902306  0.029515  0.931868  0.855995  0.897126\n",
      "f1_p_on     0.843134  0.044981  0.886941  0.769195  0.835662\n",
      "f1_p_end    0.843730  0.045337  0.886941  0.768962  0.836292\n",
      "f1_qrs_on   0.982634  0.008273  0.990287  0.972554  0.978863\n",
      "f1_qrs_end  0.980901  0.008443  0.990213  0.969108  0.976913\n",
      "f1_t_on     0.882773  0.037549  0.924414  0.825019  0.878272\n",
      "f1_t_end    0.880666  0.039063  0.924414  0.821716  0.876755\n",
      "Number of labeled data: 10\n",
      "Training Stage\n",
      "Fold 2/5: Train labeled: 96, Train unlabeled: 23970, Val: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|██▉| 124/126 [07:19<00:07,  3.54s/it, Iter=1000, Val Loss=0.1416]\n",
      " 98%|▉| 124/126 [30:02<00:29, 14.54s/it, Iter=1000, Val Loss1=0.1190, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5: Train labeled: 96, Train unlabeled: 23970, Val: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|██▉| 124/126 [07:20<00:07,  3.56s/it, Iter=1000, Val Loss=0.1026]\n",
      " 98%|▉| 124/126 [30:03<00:29, 14.54s/it, Iter=1000, Val Loss1=0.0964, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5: Train labeled: 96, Train unlabeled: 23970, Val: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|██▉| 124/126 [07:20<00:07,  3.55s/it, Iter=1000, Val Loss=0.2384]\n",
      " 98%|▉| 124/126 [30:06<00:29, 14.57s/it, Iter=1000, Val Loss1=0.2311, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5: Train labeled: 96, Train unlabeled: 23970, Val: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|██▉| 124/126 [07:22<00:07,  3.57s/it, Iter=1000, Val Loss=0.1444]\n",
      " 98%|▉| 124/126 [30:04<00:29, 14.55s/it, Iter=1000, Val Loss1=0.1191, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Stage\n",
      "                mean       std       max       min     micro\n",
      "iou_p       0.674930  0.040818  0.709588  0.608927  0.671591\n",
      "iou_qrs     0.832719  0.014127  0.854455  0.815470  0.826534\n",
      "iou_t       0.763983  0.031715  0.810767  0.726340  0.761802\n",
      "miou        0.757211  0.023890  0.777730  0.716912  0.753309\n",
      "acc         0.895980  0.008986  0.906776  0.882516  0.894465\n",
      "ave_f1      0.916965  0.017226  0.935150  0.891209  0.914637\n",
      "f1_p_on     0.864705  0.036497  0.907457  0.812914  0.861329\n",
      "f1_p_end    0.865101  0.037112  0.908299  0.811761  0.861718\n",
      "f1_qrs_on   0.980763  0.003147  0.984172  0.976485  0.979684\n",
      "f1_qrs_end  0.979066  0.004496  0.983726  0.972525  0.977158\n",
      "f1_t_on     0.905944  0.019208  0.928054  0.884620  0.903749\n",
      "f1_t_end    0.906213  0.019790  0.929053  0.885606  0.904185\n",
      "Number of labeled data: 20\n",
      "Training Stage\n",
      "Fold 1/5: Train labeled: 192, Train unlabeled: 23970, Val: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|████▉| 62/63 [07:08<00:06,  6.92s/it, Iter=1000, Val Loss=0.1158]\n",
      " 98%|▉| 62/63 [29:30<00:28, 28.56s/it, Iter=1000, Val Loss1=0.0992, Va\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5: Train labeled: 190, Train unlabeled: 23970, Val: 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|████▉| 66/67 [07:11<00:06,  6.53s/it, Iter=1000, Val Loss=0.1479]\n",
      " 99%|▉| 66/67 [29:41<00:26, 26.99s/it, Iter=1000, Val Loss1=0.1384, Va\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5: Train labeled: 192, Train unlabeled: 23970, Val: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|████▉| 62/63 [07:12<00:06,  6.98s/it, Iter=1000, Val Loss=0.0633]\n",
      " 98%|▉| 62/63 [29:43<00:28, 28.77s/it, Iter=1000, Val Loss1=0.0623, Va\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5: Train labeled: 192, Train unlabeled: 23970, Val: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|████▉| 62/63 [07:15<00:07,  7.02s/it, Iter=1000, Val Loss=0.1667]\n",
      " 98%|▉| 62/63 [29:35<00:28, 28.64s/it, Iter=1000, Val Loss1=0.1497, Va\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5: Train labeled: 192, Train unlabeled: 23970, Val: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|████▉| 62/63 [07:13<00:06,  6.99s/it, Iter=1000, Val Loss=0.0979]\n",
      " 98%|▉| 62/63 [29:40<00:28, 28.71s/it, Iter=1000, Val Loss1=0.0922, Va\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Stage\n",
      "                mean       std       max       min     micro\n",
      "iou_p       0.724391  0.034063  0.745769  0.664049  0.719862\n",
      "iou_qrs     0.851800  0.014756  0.871721  0.833039  0.850735\n",
      "iou_t       0.789371  0.022893  0.825071  0.766566  0.789772\n",
      "miou        0.788521  0.021052  0.814187  0.756479   0.78679\n",
      "acc         0.907968  0.008426  0.920775  0.898117  0.907333\n",
      "ave_f1      0.922450  0.018117  0.945488  0.898769  0.920805\n",
      "f1_p_on     0.883404  0.040654  0.910360  0.813487  0.878722\n",
      "f1_p_end    0.883422  0.040157  0.910327  0.814022  0.878722\n",
      "f1_qrs_on   0.981309  0.005562  0.987116  0.974619  0.981141\n",
      "f1_qrs_end  0.981213  0.005861  0.987941  0.976131  0.980916\n",
      "f1_t_on     0.902477  0.024540  0.937306  0.872264  0.902425\n",
      "f1_t_end    0.902873  0.024821  0.940286  0.874188  0.902902\n",
      "Number of labeled data: 50\n",
      "Training Stage\n",
      "Fold 1/5: Train labeled: 479, Train unlabeled: 23970, Val: 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|████▊| 25/26 [07:10<00:17, 17.24s/it, Iter=1000, Val Loss=0.0959]\n",
      " 96%|▉| 25/26 [29:26<01:10, 70.64s/it, Iter=1000, Val Loss1=0.0900, Va\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5: Train labeled: 477, Train unlabeled: 23970, Val: 119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|████▊| 25/26 [07:10<00:17, 17.23s/it, Iter=1000, Val Loss=0.1456]\n",
      " 96%|▉| 25/26 [29:30<01:10, 70.81s/it, Iter=1000, Val Loss1=0.1330, Va\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5: Train labeled: 474, Train unlabeled: 23970, Val: 119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|████▊| 25/26 [07:10<00:17, 17.23s/it, Iter=1000, Val Loss=0.0807]\n",
      " 96%|▉| 25/26 [29:23<01:10, 70.54s/it, Iter=1000, Val Loss1=0.0762, Va\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5: Train labeled: 478, Train unlabeled: 23970, Val: 119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|████▊| 25/26 [07:09<00:17, 17.19s/it, Iter=1000, Val Loss=0.1186]\n",
      " 96%|▉| 25/26 [29:40<01:11, 71.22s/it, Iter=1000, Val Loss1=0.0982, Va\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5: Train labeled: 474, Train unlabeled: 23970, Val: 119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|████▊| 25/26 [07:11<00:17, 17.24s/it, Iter=1000, Val Loss=0.1258]\n",
      " 96%|▉| 25/26 [29:22<01:10, 70.50s/it, Iter=1000, Val Loss1=0.1139, Va\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Stage\n",
      "                mean       std       max       min     micro\n",
      "iou_p       0.756787  0.019218  0.782320  0.741078  0.751199\n",
      "iou_qrs     0.870373  0.014751  0.889649  0.852953  0.869125\n",
      "iou_t       0.796157  0.018259  0.817587  0.770363  0.796968\n",
      "miou        0.807772  0.016599  0.826598  0.789459  0.805764\n",
      "acc         0.916096  0.007330  0.925887  0.907930  0.914433\n",
      "ave_f1      0.925689  0.008332  0.936405  0.913632  0.924212\n",
      "f1_p_on     0.895723  0.027312  0.924596  0.863389  0.890855\n",
      "f1_p_end    0.895370  0.026927  0.924596  0.862613   0.89047\n",
      "f1_qrs_on   0.984330  0.003161  0.987333  0.979813  0.984388\n",
      "f1_qrs_end  0.984238  0.002916  0.987331  0.980059  0.983892\n",
      "f1_t_on     0.896607  0.013940  0.911434  0.877533  0.897267\n",
      "f1_t_end    0.897867  0.013719  0.912137  0.880806  0.898403\n",
      "Number of labeled data: 160\n",
      "Training Stage\n",
      "Fold 1/5: Train labeled: 1536, Train unlabeled: 23970, Val: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|██████▏| 7/8 [07:36<01:05, 65.18s/it, Iter=1000, Val Loss=0.1287]\n",
      " 88%|▉| 7/8 [30:11<04:18, 258.76s/it, Iter=1000, Val Loss1=0.1168, Val\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5: Train labeled: 1536, Train unlabeled: 23970, Val: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|██████▏| 7/8 [07:37<01:05, 65.34s/it, Iter=1000, Val Loss=0.1091]\n",
      " 88%|▉| 7/8 [30:30<04:21, 261.46s/it, Iter=1000, Val Loss1=0.1081, Val\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5: Train labeled: 1536, Train unlabeled: 23970, Val: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|██████▏| 7/8 [07:37<01:05, 65.32s/it, Iter=1000, Val Loss=0.0979]\n",
      " 88%|▉| 7/8 [30:12<04:18, 258.91s/it, Iter=1000, Val Loss1=0.0979, Val\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5: Train labeled: 1536, Train unlabeled: 23970, Val: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|██████▏| 7/8 [07:35<01:05, 65.14s/it, Iter=1000, Val Loss=0.0846]\n",
      " 88%|▉| 7/8 [30:39<04:22, 262.77s/it, Iter=1000, Val Loss1=0.0786, Val\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5: Train labeled: 1536, Train unlabeled: 23970, Val: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|██████▏| 7/8 [07:37<01:05, 65.39s/it, Iter=1000, Val Loss=0.0963]\n",
      " 88%|▉| 7/8 [30:15<04:19, 259.35s/it, Iter=1000, Val Loss1=0.0917, Val\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Stage\n",
      "                mean       std       max       min     micro\n",
      "iou_p       0.771656  0.024890  0.791474  0.737266  0.766187\n",
      "iou_qrs     0.878550  0.016620  0.899821  0.856955  0.877276\n",
      "iou_t       0.811637  0.017688  0.837510  0.796128  0.812247\n",
      "miou        0.820615  0.014458  0.842077  0.803021   0.81857\n",
      "acc         0.921159  0.006984  0.932516  0.916084  0.920253\n",
      "ave_f1      0.927909  0.008082  0.936183  0.917255  0.926874\n",
      "f1_p_on     0.895075  0.021680  0.928172  0.874530  0.891557\n",
      "f1_p_end    0.895416  0.021401  0.928172  0.875644  0.891885\n",
      "f1_qrs_on   0.981458  0.007389  0.989147  0.971532  0.981219\n",
      "f1_qrs_end  0.980753  0.007942  0.988918  0.971532  0.980099\n",
      "f1_t_on     0.906710  0.006212  0.913568  0.899171  0.907646\n",
      "f1_t_end    0.908041  0.005962  0.918201  0.903378  0.908839\n"
     ]
    }
   ],
   "source": [
    "th_delineation = 150\n",
    "gpu = 2\n",
    "aug = 2\n",
    "deep_supervision = 0\n",
    "torch.cuda.set_device(gpu)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Parameters\n",
    "model_save_path = f'./checkpoints/unet_a_ds{int(deep_supervision)}_ABD.cross'\n",
    "metrics_save_path = f'./metrics/unet_a_ds{int(deep_supervision)}_ABD.cross'\n",
    "records_list = [5,10,20,50,160]\n",
    "val_ratio = 0.2\n",
    "df_list = []\n",
    "\n",
    "for num_labeled in records_list:\n",
    "    ## Train\n",
    "    print(f\"Number of labeled data: {num_labeled}\")\n",
    "    print(\"Training Stage\")\n",
    "    for fold in range(5):\n",
    "        if os.path.exists(f\"{model_save_path}.num_labeled_{num_labeled}_fold_{fold}.final_1.pth\"):\n",
    "            continue\n",
    "        # Set random seed for reproducibility\n",
    "        seed = 42\n",
    "        cudnn.benchmark = False\n",
    "        cudnn.deterministic = True\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "        x_train_l, y_train_l, _, _, _ = raw_data_load_ludb(40, num_labeled, fold, crop=[1250, 3750])\n",
    "        x_train_u, y_train_u, _, _, _ = raw_data_load_rdb(400, 1999, fold, crop=[1250, 3750])\n",
    "\n",
    "        num_val =  np.round(x_train_l.shape[0] * val_ratio).astype(int)\n",
    "        x_val, y_val = x_train_l[:num_val], y_train_l[:num_val]\n",
    "        x_train_l, y_train_l = x_train_l[num_val:], y_train_l[num_val:]\n",
    "\n",
    "        print(f\"Fold {fold+1}/{5}: Train labeled: {x_train_l.shape[0]}, Train unlabeled: {x_train_u.shape[0]}, Val: {x_val.shape[0]}\")\n",
    "\n",
    "        db_train_l = ECGDataset_ABD(x_train_l, y_train_l, weak_strong_augment=WeakStrongAugment())\n",
    "        db_train_u = ECGDataset_ABD(x_train_u, y_train_u, weak_strong_augment=WeakStrongAugment())\n",
    "        db_val = ECGDataset_ABD(x_val, y_val, transform=base_transforms())\n",
    "\n",
    "        model = UNet1D_A(length=2500, base_channels=16, kernel_size=9, dropout='channels', droprate=.2, num_classes=2, logits=True).to('cuda')\n",
    "        ini_ds, ini_aug = int(deep_supervision), int(aug)\n",
    "        model_load_path = f\"./checkpoints/unet_a_ds{ini_ds}.num_labeled_{num_labeled}_aug_{ini_aug}.fold_{fold}.epoch_20.pth\"\n",
    "        model.load_state_dict(torch.load(model_load_path))\n",
    "        \n",
    "        snapshot_path = f\"{model_save_path}.num_labeled_{num_labeled}_fold_{fold}.pre_train.pth\"\n",
    "        pre_train(model, snapshot_path, db_train_l, db_train_u, db_val)\n",
    "\n",
    "        model_1 = copy.deepcopy(model)\n",
    "        model_2 = copy.deepcopy(model)\n",
    "        ema_model = copy.deepcopy(model)\n",
    "        final_path_1 = f\"{model_save_path}.num_labeled_{num_labeled}_fold_{fold}.final_1.pth\"\n",
    "        final_path_2 = f\"{model_save_path}.num_labeled_{num_labeled}_fold_{fold}.final_2.pth\"\n",
    "        self_train(model_1, model_2, ema_model, snapshot_path, final_path_1, final_path_2, db_train_l, db_train_u, db_val)\n",
    "\n",
    "    ## Test\n",
    "    print(\"Test Stage\")\n",
    "    data = []\n",
    "    label = []\n",
    "    preds = []\n",
    "    seg_metrics_macro = []\n",
    "    deli_metrics_macro = []\n",
    "    \n",
    "    for fold in range(5):\n",
    "        model = UNet1D_A(length=2500, base_channels=16, kernel_size=9, dropout='channels', droprate=.2, num_classes=2, logits=True).to(device)\n",
    "        model_load_path = f\"{model_save_path}.num_labeled_{num_labeled}_fold_{fold}.final_1.pth\"\n",
    "        model.load_state_dict(torch.load(model_load_path))\n",
    "\n",
    "        x_train, y_train, _, x_test, y_test = raw_data_load_ludb(40, 160, fold, crop=[0, 5000])\n",
    "        test_dataset = ECGDataset(x_test, y_test, transform=base_transforms())\n",
    "\n",
    "        pred = model_predict(model, model_load_path, test_dataset, device, multi_lead_correction=False, logits=True)\n",
    "        \n",
    "        flag_ludb = np.load('./dataset/ludb/flag.npy')\n",
    "        index_shuffled_5fold = np.load('./dataset/ludb/ludb_index_shuffled_5fold_250113.npy')\n",
    "        index_shuffled = index_shuffled_5fold[:,fold]\n",
    "        index_shuffled_lead = []\n",
    "        for i in np.array(index_shuffled):\n",
    "            index_shuffled_lead.extend([k for k in range(12*i,12*i+12,1)])\n",
    "        num_test = 40\n",
    "        flag_test= flag_ludb[index_shuffled_lead[0:num_test*12]]\n",
    "        dataset = (x_test, y_test, np.zeros((x_test.shape[0],)), flag_test)\n",
    "        _, _, seg_metrics, deli_metrics = dataset_eval(dataset, pred, th_delineation=th_delineation, verbose=0)\n",
    "\n",
    "        data.append(x_test)\n",
    "        label.append(y_test)\n",
    "        preds.append(pred)\n",
    "        seg_metrics_macro.append(seg_metrics)\n",
    "        deli_metrics_macro.append(deli_metrics)\n",
    "\n",
    "    data = np.concatenate(data, axis=0)\n",
    "    label = np.concatenate(label, axis=0)\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "\n",
    "    def summarize_total_rows(dfs):\n",
    "        \"\"\"\n",
    "        Extracts 'Total' rows from DataFrames, calculates summary statistics,\n",
    "        and returns a new DataFrame.\n",
    "\n",
    "        Args:\n",
    "        dfs: A list of pandas DataFrames with identical structure.\n",
    "\n",
    "        Returns:\n",
    "            A pandas DataFrame containing the mean, std, max, and min\n",
    "            of each column of the 'Total' rows from all input DataFrames.\n",
    "        \"\"\"\n",
    "        total_rows = [df[df['type'] == 'Total'].iloc[0] for df in dfs]\n",
    "        total_df = pd.DataFrame(total_rows)\n",
    "\n",
    "        # Get original headers, and remove 'type'\n",
    "        original_headers = total_df.columns.tolist()\n",
    "        original_headers.remove('type')\n",
    "\n",
    "\n",
    "        # Calculate summary stats for each column (excluding type)\n",
    "        summary_data = {\n",
    "            'mean': total_df[original_headers].mean().to_list(),\n",
    "            'std': total_df[original_headers].std().to_list(),\n",
    "            'max': total_df[original_headers].max().to_list(),\n",
    "            'min': total_df[original_headers].min().to_list()\n",
    "        }\n",
    "        # Create the summary DataFrame\n",
    "        summary_df = pd.DataFrame(summary_data, index = original_headers)\n",
    "        return summary_df\n",
    "    \n",
    "    # Macro average metrics of 5 folds\n",
    "    seg_metrics_macro = summarize_total_rows(seg_metrics_macro)\n",
    "    deli_metrics_macro = summarize_total_rows(deli_metrics_macro) \n",
    "    filtered_df = deli_metrics_macro[deli_metrics_macro.index.str.contains('f1')]\n",
    "    merged_df_macro = pd.concat([seg_metrics_macro, filtered_df], axis = 0)                \n",
    "\n",
    "    # Micro average metrics of 5 folds\n",
    "    dataset = (data, label, np.zeros((data.shape[0],)), np.zeros((data.shape[0],)))\n",
    "    _, _, seg_metrics_micro, deli_metrics_micro = dataset_eval(dataset, preds, th_delineation=th_delineation, verbose=0)\n",
    "    # Filter df2 to include rows where column name contain 'f1'\n",
    "    filtered_df = deli_metrics_micro[['type'] + [col for col in deli_metrics_micro.columns if 'f1' in col]]\n",
    "    merged_df_micro = pd.merge(seg_metrics_micro, filtered_df, on='type', how='outer')\n",
    "    micro_row = merged_df_micro[merged_df_micro['type'] == 'Total'].iloc[0]\n",
    "    # Remove 'type' and convert to series\n",
    "    micro_row_values = micro_row.drop('type')\n",
    "    merged_df = copy.deepcopy(merged_df_macro)\n",
    "    merged_df['micro'] = micro_row_values\n",
    "    \n",
    "    df_list.append(merged_df)\n",
    "    # Final results\n",
    "    print(merged_df)\n",
    "\n",
    "\n",
    "# Save results\n",
    "# 拼接数据，将总标题作为列上方的“标题行”\n",
    "concat_frames = []\n",
    "for i, df in enumerate(df_list):\n",
    "    # 插入标题行\n",
    "    df_with_title = df.copy()\n",
    "    df_with_title.columns = pd.MultiIndex.from_tuples([(str(records_list[i]), col) for col in df.columns])\n",
    "    concat_frames.append(df_with_title)\n",
    "\n",
    "# 按列拼接，并保留行标题\n",
    "result = pd.concat(concat_frames, axis=1)\n",
    "\n",
    "# 写入 Excel\n",
    "result.to_excel(f\"{metrics_save_path}.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
