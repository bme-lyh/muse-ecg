{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, LambdaLR\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from network.model_unet_a_2d import *\n",
    "from loss_utils import *\n",
    "from data_loader import *\n",
    "from data_augmentation import *\n",
    "from test_utils import model_predict, dataset_eval\n",
    "from torchinfo import summary\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data.sampler import Sampler\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_once(iterable):\n",
    "    return np.random.permutation(iterable)\n",
    "\n",
    "\n",
    "def iterate_eternally(indices):\n",
    "    def infinite_shuffles():\n",
    "        while True:\n",
    "            yield np.random.permutation(indices)\n",
    "    return itertools.chain.from_iterable(infinite_shuffles())\n",
    "\n",
    "\n",
    "def grouper(iterable, n):\n",
    "    \"Collect data into fixed-length chunks or blocks\"\n",
    "    # grouper('ABCDEFG', 3) --> ABC DEF\"\n",
    "    args = [iter(iterable)] * n\n",
    "    return zip(*args)\n",
    "\n",
    "\n",
    "class TwoStreamBatchSampler(Sampler):\n",
    "    \"\"\"Iterate two sets of indices\n",
    "\n",
    "    An 'epoch' is one iteration through the primary indices.\n",
    "    During the epoch, the secondary indices are iterated through\n",
    "    as many times as needed.\n",
    "    \"\"\"\n",
    "    def __init__(self, primary_indices, secondary_indices, batch_size, secondary_batch_size):\n",
    "        self.primary_indices = primary_indices\n",
    "        self.secondary_indices = secondary_indices\n",
    "        self.secondary_batch_size = secondary_batch_size\n",
    "        self.primary_batch_size = batch_size - secondary_batch_size\n",
    "\n",
    "        assert len(self.primary_indices) >= self.primary_batch_size > 0\n",
    "        assert len(self.secondary_indices) >= self.secondary_batch_size > 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        primary_iter = iterate_once(self.primary_indices)\n",
    "        secondary_iter = iterate_eternally(self.secondary_indices)\n",
    "        return (\n",
    "            primary_batch + secondary_batch\n",
    "            for (primary_batch, secondary_batch)\n",
    "            in zip(grouper(primary_iter, self.primary_batch_size),\n",
    "                    grouper(secondary_iter, self.secondary_batch_size))\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.primary_indices) // self.primary_batch_size\n",
    "\n",
    "\n",
    "def generate_mask(img):\n",
    "    \"\"\"\n",
    "    随机生成遮罩，返回遮罩及对应的损失遮罩。\n",
    "    针对 H=1 的数据，确保 patch_x 至少为1，并处理随机起始位置的边界条件。\n",
    "    \"\"\"\n",
    "    batch_size, channel, img_x, img_y = img.shape\n",
    "    loss_mask = torch.ones(batch_size, img_x, img_y).cuda()\n",
    "    mask = torch.ones(img_x, img_y).cuda()\n",
    "    # 如果 img_x == 1，则 force patch_x 为1；否则按比例计算\n",
    "    patch_x = int(img_x * 2 / 3) if img_x > 1 else 1\n",
    "    patch_y = int(img_y * 2 / 3)\n",
    "    # 计算随机起始位置时，防止负值\n",
    "    if img_x - patch_x <= 0:\n",
    "        w = 0\n",
    "    else:\n",
    "        w = np.random.randint(0, img_x - patch_x)\n",
    "    if img_y - patch_y <= 0:\n",
    "        h = 0\n",
    "    else:\n",
    "        h = np.random.randint(0, img_y - patch_y)\n",
    "    mask[w:w + patch_x, h:h + patch_y] = 0\n",
    "    loss_mask[:, w:w + patch_x, h:h + patch_y] = 0\n",
    "    return mask.long(), loss_mask.long()\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def _one_hot_encoder(self, input_tensor):\n",
    "        tensor_list = []\n",
    "        for i in range(self.n_classes):\n",
    "            temp_prob = input_tensor == i * torch.ones_like(input_tensor)\n",
    "            tensor_list.append(temp_prob)\n",
    "        output_tensor = torch.cat(tensor_list, dim=1)\n",
    "        return output_tensor.float()\n",
    "\n",
    "    def _dice_loss(self, score, target):\n",
    "        target = target.float()\n",
    "        smooth = 1e-10\n",
    "        intersect = torch.sum(score * target)\n",
    "        y_sum = torch.sum(target * target)\n",
    "        z_sum = torch.sum(score * score)\n",
    "        loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth)\n",
    "        loss = 1 - loss\n",
    "        return loss\n",
    "    \n",
    "    def _dice_mask_loss(self, score, target, mask):\n",
    "        target = target.float()\n",
    "        mask = mask.float()\n",
    "        smooth = 1e-10\n",
    "        intersect = torch.sum(score * target * mask)\n",
    "        y_sum = torch.sum(target * target * mask)\n",
    "        z_sum = torch.sum(score * score * mask)\n",
    "        loss = (2 * intersect + smooth ) / (z_sum + y_sum + smooth)\n",
    "        loss = 1 - loss\n",
    "        return loss\n",
    "\n",
    "    def forward(self, inputs, target, mask=None, weight=None, softmax=False, one_hot=False):\n",
    "        # If softmax is True, apply softmax to the inputs\n",
    "        if softmax:\n",
    "            inputs = torch.softmax(inputs, dim=1)\n",
    "\n",
    "        # One-hot encoding of target, only if one_hot is True\n",
    "        if one_hot:\n",
    "            target = self._one_hot_encoder(target)\n",
    "        \n",
    "        if weight is None:\n",
    "            weight = [1] * self.n_classes\n",
    "\n",
    "        assert inputs.size() == target.size(), 'predict & target shape do not match'\n",
    "        \n",
    "        class_wise_dice = []\n",
    "        loss = 0.0\n",
    "        if mask is not None:\n",
    "            # Expand mask to match the number of classes\n",
    "            mask = mask.repeat(1, self.n_classes, 1, 1).type(torch.float32)\n",
    "            for i in range(0, self.n_classes): \n",
    "                dice = self._dice_mask_loss(inputs[:, i], target[:, i], mask[:, i])\n",
    "                class_wise_dice.append(1.0 - dice.item())\n",
    "                loss += dice * weight[i]\n",
    "        else:\n",
    "            for i in range(0, self.n_classes):\n",
    "                dice = self._dice_loss(inputs[:, i], target[:, i])\n",
    "                class_wise_dice.append(1.0 - dice.item())\n",
    "                loss += dice * weight[i]\n",
    "        \n",
    "        return loss / self.n_classes\n",
    "    \n",
    "\n",
    "def mix_loss(output, img_l, patch_l, mask, l_weight=1.0, u_weight=0.5, unlab=False):\n",
    "    CE = nn.CrossEntropyLoss(reduction='none')\n",
    "    dice_loss = DiceLoss(n_classes=4)\n",
    "    img_l, patch_l = img_l.type(torch.int64), patch_l.type(torch.int64)\n",
    "    output_soft = F.softmax(output, dim=1)\n",
    "    image_weight, patch_weight = l_weight, u_weight\n",
    "    if unlab:\n",
    "        image_weight, patch_weight = u_weight, l_weight\n",
    "    patch_mask = 1 - mask\n",
    "    loss_dice = dice_loss(output_soft, img_l, mask.unsqueeze(1)) * image_weight\n",
    "    loss_dice += dice_loss(output_soft, patch_l, patch_mask.unsqueeze(1)) * patch_weight\n",
    "    # loss_ce = image_weight * (CE(output, img_l) * mask).sum() / (mask.sum() + 1e-16) \n",
    "    # loss_ce += patch_weight * (CE(output, patch_l) * patch_mask).sum() / (patch_mask.sum() + 1e-16)#loss = loss_ce\n",
    "    loss_ce = image_weight * (CE(output, img_l.argmax(dim=1)) * mask).sum() / (mask.sum() + 1e-16) \n",
    "    loss_ce += patch_weight * (CE(output, patch_l.argmax(dim=1)) * patch_mask).sum() / (patch_mask.sum() + 1e-16)#loss = loss_ce\n",
    "    return loss_dice, loss_ce\n",
    "\n",
    "\n",
    "def pre_train(model, snapshot_path, db_train_l, db_train_u, db_val):\n",
    "    base_lr = 0.01\n",
    "    num_classes = 4\n",
    "    # max_iterations = 10000\n",
    "    max_iterations = 1000\n",
    "    batch_size = 24\n",
    "    labeled_bs = 12\n",
    "    seed = 42\n",
    "    labeled_sub_bs, unlabeled_sub_bs = int(labeled_bs/2), int((batch_size-labeled_bs) / 2)\n",
    "\n",
    "    def worker_init_fn(worker_id):\n",
    "        random.seed(seed + worker_id)\n",
    "\n",
    "    db_train = torch.utils.data.ConcatDataset([db_train_l, db_train_u])\n",
    "    num_total = len(db_train)\n",
    "    num_labeled = len(db_train_l)\n",
    "    labeled_idxs = list(range(0, num_labeled))\n",
    "    unlabeled_idxs = list(range(num_labeled, num_total))\n",
    "    batch_sampler = TwoStreamBatchSampler(labeled_idxs, unlabeled_idxs, batch_size, batch_size-labeled_bs)\n",
    "\n",
    "    trainloader = DataLoader(db_train, batch_sampler=batch_sampler, num_workers=4, pin_memory=True, worker_init_fn=worker_init_fn)\n",
    "\n",
    "    valloader = DataLoader(db_val, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=base_lr, momentum=0.9, weight_decay=0.0001)\n",
    "\n",
    "    model.train()\n",
    "    iter_num = 0\n",
    "    max_epoch = max_iterations // len(trainloader) + 1\n",
    "    best_val_loss = np.inf\n",
    "    iterator = tqdm(range(max_epoch), ncols=70)\n",
    "    for _ in iterator:\n",
    "        for _, (volume_batch, label_batch) in enumerate(trainloader):\n",
    "            volume_batch, label_batch = volume_batch.cuda(), label_batch.cuda()\n",
    "\n",
    "            img_a, img_b = volume_batch[:labeled_sub_bs], volume_batch[labeled_sub_bs:labeled_bs]\n",
    "            lab_a, lab_b = label_batch[:labeled_sub_bs], label_batch[labeled_sub_bs:labeled_bs]\n",
    "            img_mask, loss_mask = generate_mask(img_a)\n",
    "            gt_mixl = lab_a * img_mask + lab_b * (1 - img_mask)\n",
    "\n",
    "            #-- original\n",
    "            net_input = img_a * img_mask + img_b * (1 - img_mask)\n",
    "            out_mixl = model(net_input)\n",
    "            loss_dice, loss_ce = mix_loss(out_mixl, lab_a, lab_b, loss_mask, u_weight=1.0, unlab=True)\n",
    "\n",
    "            loss = (loss_dice + loss_ce) / 2            \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            iter_num += 1\n",
    "\n",
    "            if iter_num > 0 and iter_num % 200 == 0:\n",
    "                model.eval()\n",
    "                val_loss = 0\n",
    "                for _, sampled_batch in enumerate(valloader):\n",
    "                    val_volume, val_label = sampled_batch\n",
    "                    val_volume, val_label = val_volume.cuda(), val_label.cuda()\n",
    "                    val_output = model(val_volume)\n",
    "                    val_loss += DiceLoss(n_classes=4)(val_output, val_label).item()\n",
    "                val_loss /= len(valloader)\n",
    "                iterator.set_postfix({'Iter': iter_num, 'Val Loss': f'{val_loss:.4f}'})\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    torch.save(model.state_dict(), snapshot_path)\n",
    "\n",
    "                model.train()\n",
    "\n",
    "            if iter_num >= max_iterations:\n",
    "                break\n",
    "        if iter_num >= max_iterations:\n",
    "            iterator.close()\n",
    "            break\n",
    "\n",
    "\n",
    "def get_ACDC_masks(output):\n",
    "    # probs = F.softmax(output, dim=1)\n",
    "    _, probs = torch.max(output, dim=1)\n",
    "    probs = F.one_hot(probs.squeeze(1).squeeze(1), num_classes=4).permute(0, 2, 1).unsqueeze(2).float()\n",
    "    # if nms == 1:\n",
    "    #     probs = get_ACDC_2DLargestCC(probs)      \n",
    "    return probs\n",
    "\n",
    "def sigmoid_rampup(current, rampup_length):\n",
    "    \"\"\"Exponential rampup from https://arxiv.org/abs/1610.02242\"\"\"\n",
    "    if rampup_length == 0:\n",
    "        return 1.0\n",
    "    else:               \n",
    "        current = np.clip(current, 0.0, rampup_length)\n",
    "        phase = 1.0 - current / rampup_length\n",
    "        return float(np.exp(-5.0 * phase * phase))\n",
    "    \n",
    "def get_current_consistency_weight(epoch):\n",
    "    # Consistency ramp-up from https://arxiv.org/abs/1610.02242\n",
    "    consistency = 0.1\n",
    "    consistency_rampup = 200.0\n",
    "    return 5* consistency * sigmoid_rampup(epoch, consistency_rampup)\n",
    "\n",
    "def update_model_ema(model, ema_model, alpha):\n",
    "    model_state = model.state_dict()\n",
    "    model_ema_state = ema_model.state_dict()\n",
    "    new_dict = {}\n",
    "    for key in model_state:\n",
    "        new_dict[key] = alpha * model_ema_state[key] + (1 - alpha) * model_state[key]\n",
    "    ema_model.load_state_dict(new_dict)\n",
    "\n",
    "\n",
    "def self_train(model, ema_model, snapshot_path, final_path, db_train_l, db_train_u, db_val):\n",
    "    base_lr = 0.01\n",
    "    num_classes = 4\n",
    "    # max_iterations = 30000\n",
    "    max_iterations = 1000\n",
    "    batch_size = 24\n",
    "    labeled_bs = 12\n",
    "    seed = 42\n",
    "    u_weight = 0.5\n",
    "    labeled_sub_bs, unlabeled_sub_bs = int(labeled_bs/2), int((batch_size-labeled_bs) / 2)\n",
    "\n",
    "    def worker_init_fn(worker_id):\n",
    "        random.seed(seed + worker_id)\n",
    "\n",
    "    db_train = torch.utils.data.ConcatDataset([db_train_l, db_train_u])\n",
    "    num_total = len(db_train)\n",
    "    num_labeled = len(db_train_l)\n",
    "    labeled_idxs = list(range(0, num_labeled))\n",
    "    unlabeled_idxs = list(range(num_labeled, num_total))\n",
    "    batch_sampler = TwoStreamBatchSampler(labeled_idxs, unlabeled_idxs, batch_size, batch_size-labeled_bs)\n",
    "\n",
    "    trainloader = DataLoader(db_train, batch_sampler=batch_sampler, num_workers=4, pin_memory=True, worker_init_fn=worker_init_fn)\n",
    "\n",
    "    valloader = DataLoader(db_val, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=base_lr, momentum=0.9, weight_decay=0.0001)\n",
    "\n",
    "    model.load_state_dict(torch.load(snapshot_path))    \n",
    "    ema_model.load_state_dict(torch.load(snapshot_path))\n",
    "\n",
    "    model.train()\n",
    "    ema_model.train()\n",
    "\n",
    "    iter_num = 0\n",
    "    max_epoch = max_iterations // len(trainloader) + 1\n",
    "    best_val_loss = np.inf\n",
    "    iterator = tqdm(range(max_epoch), ncols=70)\n",
    "    for _ in iterator:\n",
    "        for _, sampled_batch in enumerate(trainloader):\n",
    "            volume_batch, label_batch = sampled_batch\n",
    "            volume_batch, label_batch = volume_batch.cuda(), label_batch.cuda()\n",
    "\n",
    "            img_a, img_b = volume_batch[:labeled_sub_bs], volume_batch[labeled_sub_bs:labeled_bs]\n",
    "            uimg_a, uimg_b = volume_batch[labeled_bs:labeled_bs + unlabeled_sub_bs], volume_batch[labeled_bs + unlabeled_sub_bs:]\n",
    "            ulab_a, ulab_b = label_batch[labeled_bs:labeled_bs + unlabeled_sub_bs], label_batch[labeled_bs + unlabeled_sub_bs:]\n",
    "            lab_a, lab_b = label_batch[:labeled_sub_bs], label_batch[labeled_sub_bs:labeled_bs]\n",
    "            with torch.no_grad():\n",
    "                pre_a = ema_model(uimg_a)\n",
    "                pre_b = ema_model(uimg_b)\n",
    "                plab_a = get_ACDC_masks(pre_a)\n",
    "                plab_b = get_ACDC_masks(pre_b)\n",
    "                img_mask, loss_mask = generate_mask(img_a)\n",
    "                unl_label = ulab_a * img_mask + lab_a * (1 - img_mask)\n",
    "                l_label = lab_b * img_mask + ulab_b * (1 - img_mask)\n",
    "            consistency_weight = get_current_consistency_weight(iter_num//150)\n",
    "\n",
    "            net_input_unl = uimg_a * img_mask + img_a * (1 - img_mask)\n",
    "            net_input_l = img_b * img_mask + uimg_b * (1 - img_mask)\n",
    "            out_unl = model(net_input_unl)\n",
    "            out_l = model(net_input_l)\n",
    "            unl_dice, unl_ce = mix_loss(out_unl, plab_a, lab_a, loss_mask, u_weight=u_weight, unlab=True)\n",
    "            l_dice, l_ce = mix_loss(out_l, lab_b, plab_b, loss_mask, u_weight=u_weight)\n",
    "\n",
    "\n",
    "            loss_ce = unl_ce + l_ce \n",
    "            loss_dice = unl_dice + l_dice\n",
    "\n",
    "            loss = (loss_dice + loss_ce) / 2            \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            iter_num += 1\n",
    "            update_model_ema(model, ema_model, 0.99)\n",
    "\n",
    "            if iter_num > 0 and iter_num % 200 == 0:\n",
    "                model.eval()\n",
    "                val_loss = 0\n",
    "                for _, sampled_batch in enumerate(valloader):\n",
    "                    val_volume, val_label = sampled_batch\n",
    "                    val_volume, val_label = val_volume.cuda(), val_label.cuda()\n",
    "                    val_output = model(val_volume)\n",
    "                    val_loss += DiceLoss(n_classes=4)(val_output, val_label).item()\n",
    "                val_loss /= len(valloader)\n",
    "                # print('Iter %d, Val Loss: %.4f' % (iter_num, val_loss))\n",
    "                iterator.set_postfix({'Iter': iter_num, 'Val Loss': f'{val_loss:.4f}'})\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    torch.save(model.state_dict(), final_path)\n",
    "                model.train()\n",
    "\n",
    "            if iter_num >= max_iterations:\n",
    "                break\n",
    "        if iter_num >= max_iterations:\n",
    "            iterator.close()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labeled data: 5\n",
      "Training Stage\n",
      "Fold 1/5: Train labeled: 48, Train unlabeled: 23970, Val: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|██▉| 124/126 [07:02<00:06,  3.41s/it, Iter=1000, Val Loss=0.0886]\n",
      " 98%|██▉| 124/126 [14:53<00:14,  7.21s/it, Iter=1000, Val Loss=0.0992]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5: Train labeled: 48, Train unlabeled: 23970, Val: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|██▉| 124/126 [07:01<00:06,  3.40s/it, Iter=1000, Val Loss=0.1690]\n",
      " 98%|██▉| 124/126 [14:56<00:14,  7.23s/it, Iter=1000, Val Loss=0.1420]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5: Train labeled: 48, Train unlabeled: 23970, Val: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|██▉| 124/126 [07:02<00:06,  3.41s/it, Iter=1000, Val Loss=0.1680]\n",
      " 98%|██▉| 124/126 [15:01<00:14,  7.27s/it, Iter=1000, Val Loss=0.1669]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5: Train labeled: 48, Train unlabeled: 23970, Val: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|██▉| 124/126 [06:59<00:06,  3.39s/it, Iter=1000, Val Loss=0.3977]\n",
      " 98%|██▉| 124/126 [14:51<00:14,  7.19s/it, Iter=1000, Val Loss=0.3996]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5: Train labeled: 48, Train unlabeled: 23970, Val: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|██▉| 124/126 [07:00<00:06,  3.39s/it, Iter=1000, Val Loss=0.1547]\n",
      " 98%|██▉| 124/126 [14:50<00:14,  7.18s/it, Iter=1000, Val Loss=0.1766]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Stage\n",
      "                mean       std       max       min     micro\n",
      "iou_p       0.715074  0.023598  0.731933  0.674456  0.706936\n",
      "iou_qrs     0.849081  0.017706  0.870191  0.827223  0.844245\n",
      "iou_t       0.788883  0.032004  0.821150  0.748058  0.786814\n",
      "miou        0.784346  0.018942  0.804991  0.767919  0.779332\n",
      "acc         0.900806  0.010991  0.911183  0.883894  0.898938\n",
      "ave_f1      0.951976  0.006614  0.959019  0.942642  0.949369\n",
      "f1_p_on     0.918308  0.010448  0.933457  0.904556  0.912309\n",
      "f1_p_end    0.919998  0.009952  0.935933  0.909170  0.913908\n",
      "f1_qrs_on   0.988653  0.003927  0.992136  0.983985   0.98767\n",
      "f1_qrs_end  0.987858  0.004757  0.992136  0.982460  0.986446\n",
      "f1_t_on     0.950126  0.010136  0.965450  0.939810  0.948997\n",
      "f1_t_end    0.946916  0.010000  0.961801  0.934530  0.946885\n",
      "Number of labeled data: 10\n",
      "Training Stage\n",
      "Fold 1/5: Train labeled: 96, Train unlabeled: 23970, Val: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|████▉| 62/63 [06:55<00:06,  6.71s/it, Iter=1000, Val Loss=0.1014]\n",
      " 98%|████▉| 62/63 [14:27<00:13, 13.99s/it, Iter=1000, Val Loss=0.0988]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5: Train labeled: 96, Train unlabeled: 23970, Val: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|████▉| 62/63 [06:56<00:06,  6.72s/it, Iter=1000, Val Loss=0.1557]\n",
      " 98%|████▉| 62/63 [14:26<00:13, 13.98s/it, Iter=1000, Val Loss=0.1426]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5: Train labeled: 96, Train unlabeled: 23970, Val: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|████▉| 62/63 [06:56<00:06,  6.73s/it, Iter=1000, Val Loss=0.1115]\n",
      " 98%|████▉| 62/63 [14:28<00:14, 14.00s/it, Iter=1000, Val Loss=0.1057]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5: Train labeled: 96, Train unlabeled: 23970, Val: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|████▉| 62/63 [06:57<00:06,  6.73s/it, Iter=1000, Val Loss=0.2407]\n",
      " 98%|████▉| 62/63 [14:27<00:13, 13.99s/it, Iter=1000, Val Loss=0.2391]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5: Train labeled: 96, Train unlabeled: 23970, Val: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|████▉| 62/63 [06:56<00:06,  6.71s/it, Iter=1000, Val Loss=0.1464]\n",
      " 98%|████▉| 62/63 [14:27<00:13, 13.99s/it, Iter=1000, Val Loss=0.1346]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Stage\n",
      "                mean       std       max       min     micro\n",
      "iou_p       0.740807  0.026101  0.767621  0.701072  0.731649\n",
      "iou_qrs     0.856390  0.018554  0.878888  0.841851  0.854007\n",
      "iou_t       0.812654  0.019067  0.838660  0.791861  0.812506\n",
      "miou        0.803284  0.016228  0.821509  0.781004  0.799387\n",
      "acc         0.911107  0.006742  0.918337  0.902825  0.909599\n",
      "ave_f1      0.958141  0.006670  0.966520  0.947882  0.956289\n",
      "f1_p_on     0.924489  0.011512  0.935211  0.909986  0.918672\n",
      "f1_p_end    0.925442  0.010786  0.934943  0.912590  0.919569\n",
      "f1_qrs_on   0.988399  0.004977  0.995002  0.982771  0.988465\n",
      "f1_qrs_end  0.987613  0.005549  0.995002  0.982299   0.98738\n",
      "f1_t_on     0.962839  0.008235  0.970947  0.950013   0.96312\n",
      "f1_t_end    0.960067  0.009140  0.968018  0.945445  0.960527\n",
      "Number of labeled data: 20\n",
      "Training Stage\n",
      "Fold 1/5: Train labeled: 192, Train unlabeled: 23970, Val: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|████▊| 31/32 [06:57<00:13, 13.47s/it, Iter=1000, Val Loss=0.1239]\n",
      " 97%|████▊| 31/32 [14:24<00:27, 27.87s/it, Iter=1000, Val Loss=0.1156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5: Train labeled: 190, Train unlabeled: 23970, Val: 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|████▊| 32/33 [06:58<00:13, 13.09s/it, Iter=1000, Val Loss=0.1611]\n",
      " 97%|████▊| 32/33 [14:18<00:26, 26.84s/it, Iter=1000, Val Loss=0.1618]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5: Train labeled: 192, Train unlabeled: 23970, Val: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|████▊| 31/32 [06:58<00:13, 13.51s/it, Iter=1000, Val Loss=0.0880]\n",
      " 97%|████▊| 31/32 [14:24<00:27, 27.90s/it, Iter=1000, Val Loss=0.0902]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5: Train labeled: 192, Train unlabeled: 23970, Val: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|████▊| 31/32 [07:00<00:13, 13.56s/it, Iter=1000, Val Loss=0.1539]\n",
      " 97%|████▊| 31/32 [14:25<00:27, 27.91s/it, Iter=1000, Val Loss=0.1637]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5: Train labeled: 192, Train unlabeled: 23970, Val: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|████▊| 31/32 [07:02<00:13, 13.63s/it, Iter=1000, Val Loss=0.1145]\n",
      " 97%|████▊| 31/32 [14:22<00:27, 27.81s/it, Iter=1000, Val Loss=0.1064]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Stage\n",
      "                mean       std       max       min     micro\n",
      "iou_p       0.762053  0.028295  0.788826  0.718166  0.755452\n",
      "iou_qrs     0.857762  0.012652  0.871281  0.838328  0.856821\n",
      "iou_t       0.818833  0.023779  0.854186  0.792700   0.81909\n",
      "miou        0.812883  0.015935  0.825135  0.786118  0.810454\n",
      "acc         0.915586  0.005578  0.921860  0.909464  0.914588\n",
      "ave_f1      0.962812  0.007813  0.967988  0.949133  0.961291\n",
      "f1_p_on     0.937251  0.011357  0.947513  0.921569  0.932279\n",
      "f1_p_end    0.938313  0.011395  0.948061  0.923351  0.933303\n",
      "f1_qrs_on   0.987815  0.006653  0.992423  0.976153  0.987946\n",
      "f1_qrs_end  0.986951  0.008570  0.993111  0.971995  0.986815\n",
      "f1_t_on     0.963587  0.008609  0.970793  0.951002  0.963977\n",
      "f1_t_end    0.962954  0.008465  0.971551  0.950731  0.963426\n",
      "Number of labeled data: 50\n",
      "Training Stage\n",
      "Fold 1/5: Train labeled: 479, Train unlabeled: 23970, Val: 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|████▌| 12/13 [07:04<00:35, 35.37s/it, Iter=1000, Val Loss=0.0995]\n",
      " 92%|████▌| 12/13 [14:17<01:11, 71.47s/it, Iter=1000, Val Loss=0.1042]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5: Train labeled: 477, Train unlabeled: 23970, Val: 119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|████▌| 12/13 [07:02<00:35, 35.22s/it, Iter=1000, Val Loss=0.1344]\n",
      " 92%|████▌| 12/13 [14:15<01:11, 71.33s/it, Iter=1000, Val Loss=0.1419]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5: Train labeled: 474, Train unlabeled: 23970, Val: 119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|████▌| 12/13 [07:02<00:35, 35.20s/it, Iter=1000, Val Loss=0.0881]\n",
      " 92%|████▌| 12/13 [14:16<01:11, 71.36s/it, Iter=1000, Val Loss=0.0920]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5: Train labeled: 478, Train unlabeled: 23970, Val: 119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|████▌| 12/13 [07:03<00:35, 35.33s/it, Iter=1000, Val Loss=0.0985]\n",
      " 92%|████▌| 12/13 [14:17<01:11, 71.47s/it, Iter=1000, Val Loss=0.1048]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5: Train labeled: 474, Train unlabeled: 23970, Val: 119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|████▌| 12/13 [07:02<00:35, 35.24s/it, Iter=1000, Val Loss=0.1179]\n",
      " 92%|████▌| 12/13 [14:16<01:11, 71.35s/it, Iter=1000, Val Loss=0.1221]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Stage\n",
      "                mean       std       max       min     micro\n",
      "iou_p       0.792843  0.031633  0.820447  0.739035  0.783863\n",
      "iou_qrs     0.882800  0.005554  0.888969  0.875855  0.881214\n",
      "iou_t       0.845127  0.018903  0.867158  0.818858  0.844963\n",
      "miou        0.840257  0.015833  0.858858  0.818495   0.83668\n",
      "acc         0.927942  0.007269  0.937660  0.919470  0.926595\n",
      "ave_f1      0.972568  0.008907  0.979842  0.960964  0.970561\n",
      "f1_p_on     0.950960  0.016136  0.966142  0.927356  0.944746\n",
      "f1_p_end    0.951429  0.016006  0.966439  0.927652  0.945205\n",
      "f1_qrs_on   0.991194  0.002296  0.993808  0.988857  0.991162\n",
      "f1_qrs_end  0.991288  0.002233  0.993808  0.988857  0.990845\n",
      "f1_t_on     0.975810  0.009596  0.984046  0.965274  0.976205\n",
      "f1_t_end    0.974725  0.009541  0.982999  0.962663  0.975203\n",
      "Number of labeled data: 160\n",
      "Training Stage\n",
      "Fold 1/5: Train labeled: 1536, Train unlabeled: 23970, Val: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|████▌ | 3/4 [07:35<02:31, 151.72s/it, Iter=1000, Val Loss=0.1112]\n",
      " 75%|████▌ | 3/4 [14:44<04:54, 294.98s/it, Iter=1000, Val Loss=0.1265]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5: Train labeled: 1536, Train unlabeled: 23970, Val: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|████▌ | 3/4 [07:35<02:31, 151.83s/it, Iter=1000, Val Loss=0.1210]\n",
      " 75%|████▌ | 3/4 [14:44<04:54, 294.88s/it, Iter=1000, Val Loss=0.1231]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5: Train labeled: 1536, Train unlabeled: 23970, Val: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|████▌ | 3/4 [07:36<02:32, 152.02s/it, Iter=1000, Val Loss=0.1052]\n",
      " 75%|████▌ | 3/4 [14:44<04:54, 294.98s/it, Iter=1000, Val Loss=0.1093]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5: Train labeled: 1536, Train unlabeled: 23970, Val: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|████▌ | 3/4 [07:33<02:31, 151.33s/it, Iter=1000, Val Loss=0.0812]\n",
      " 75%|████▌ | 3/4 [14:45<04:55, 295.04s/it, Iter=1000, Val Loss=0.0856]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5: Train labeled: 1536, Train unlabeled: 23970, Val: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|████▌ | 3/4 [07:34<02:31, 151.34s/it, Iter=1000, Val Loss=0.0974]\n",
      " 75%|████▌ | 3/4 [14:44<04:54, 294.92s/it, Iter=1000, Val Loss=0.1028]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Stage\n",
      "                mean       std       max       min     micro\n",
      "iou_p       0.813831  0.016827  0.828171  0.785289  0.808216\n",
      "iou_qrs     0.892268  0.008258  0.905985  0.884357  0.890936\n",
      "iou_t       0.854198  0.017577  0.881488  0.832701  0.854311\n",
      "miou        0.853433  0.011239  0.871881  0.842286  0.851154\n",
      "acc         0.934034  0.006154  0.944245  0.927565  0.933191\n",
      "ave_f1      0.975334  0.003868  0.981258  0.971756  0.974246\n",
      "f1_p_on     0.957892  0.005840  0.963778  0.949122  0.954387\n",
      "f1_p_end    0.958137  0.005531  0.963484  0.949727  0.954617\n",
      "f1_qrs_on   0.991753  0.002680  0.994720  0.988444  0.991662\n",
      "f1_qrs_end  0.991706  0.002728  0.994720  0.988444  0.991345\n",
      "f1_t_on     0.976889  0.005471  0.985296  0.971264  0.977334\n",
      "f1_t_end    0.975626  0.007036  0.985552  0.965753  0.976131\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "th_delineation = 150\n",
    "gpu = 3\n",
    "aug = 2\n",
    "deep_supervision = 0\n",
    "torch.cuda.set_device(gpu)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Parameters\n",
    "model_save_path = f'./checkpoints/unet_a_ds{int(deep_supervision)}_aug_{aug}_BCP.cross'\n",
    "metrics_save_path = f'./metrics/unet_a_ds{int(deep_supervision)}_aug_{aug}_BCP.cross'\n",
    "records_list = [5,10,20,50,160]\n",
    "val_ratio = 0.2\n",
    "df_list = []\n",
    "\n",
    "for num_labeled in records_list:\n",
    "    ## Train\n",
    "    print(f\"Number of labeled data: {num_labeled}\")\n",
    "    print(\"Training Stage\")\n",
    "    for fold in range(5):\n",
    "        # if os.path.exists(f\"{model_save_path}.num_labeled_{num_labeled}_fold_{fold}.final.pth\"):\n",
    "        #     continue\n",
    "        # Set random seed for reproducibility\n",
    "        seed = 42\n",
    "        cudnn.benchmark = False\n",
    "        cudnn.deterministic = True\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "        x_train_l, y_train_l, _, _, _ = raw_data_load_ludb(40, num_labeled, fold, crop=[1250, 3750])\n",
    "        x_train_u, y_train_u, _, _, _ = raw_data_load_rdb(400, 1999, fold, crop=[1250, 3750])\n",
    "\n",
    "        num_val =  np.round(x_train_l.shape[0] * val_ratio).astype(int)\n",
    "        x_val, y_val = x_train_l[:num_val], y_train_l[:num_val]\n",
    "        x_train_l, y_train_l = x_train_l[num_val:], y_train_l[num_val:]\n",
    "\n",
    "        print(f\"Fold {fold+1}/{5}: Train labeled: {x_train_l.shape[0]}, Train unlabeled: {x_train_u.shape[0]}, Val: {x_val.shape[0]}\")\n",
    "\n",
    "        if aug == 0:\n",
    "            db_train_l = ECGDataset(x_train_l, y_train_l, transform=base_transforms())\n",
    "            db_train_u = ECGDataset(x_train_u, y_train_u, transform=base_transforms())\n",
    "            db_val = ECGDataset(x_val, y_val, transform=base_transforms())\n",
    "        elif aug == 1:\n",
    "            db_train_l = ECGDataset(x_train_l, y_train_l, transform=get_train_transforms())\n",
    "            db_train_u = ECGDataset(x_train_u, y_train_u, transform=get_train_transforms())\n",
    "            db_val = ECGDataset(x_val, y_val, transform=get_train_transforms())\n",
    "        elif aug == 2:\n",
    "            db_train_l = ECGDataset(x_train_l, y_train_l, transform=base_transforms())\n",
    "            db_train_l_aug = ECGDataset(x_train_l, y_train_l, transform=get_train_transforms())\n",
    "            db_train_l = torch.utils.data.ConcatDataset([db_train_l, db_train_l_aug])\n",
    "            db_train_u = ECGDataset(x_train_u, y_train_u, transform=base_transforms())\n",
    "            db_train_u_aug = ECGDataset(x_train_u, y_train_u, transform=get_train_transforms())\n",
    "            db_train_u = torch.utils.data.ConcatDataset([db_train_u, db_train_u_aug])\n",
    "            db_val = ECGDataset(x_val, y_val, transform=base_transforms())\n",
    "            db_val_aug = ECGDataset(x_val, y_val, transform=get_train_transforms())\n",
    "            db_val = torch.utils.data.ConcatDataset([db_val, db_val_aug])\n",
    "        else:\n",
    "            raise ValueError(\"Invalid aug value. Choose from 0, 1, 2\")\n",
    "\n",
    "        model = UNet1D_A(length=2500, base_channels=16, kernel_size=9, dropout='channels', droprate=.2, num_classes=2).to('cuda')\n",
    "        ini_ds, ini_aug = int(deep_supervision), int(aug)\n",
    "        model_load_path = f\"./checkpoints/unet_a_ds{ini_ds}.num_labeled_{num_labeled}_aug_{ini_aug}.fold_{fold}.epoch_20.pth\"\n",
    "        model.load_state_dict(torch.load(model_load_path))\n",
    "        \n",
    "        snapshot_path = f\"{model_save_path}.num_labeled_{num_labeled}_fold_{fold}.pre_train.pth\"\n",
    "        pre_train(model, snapshot_path, db_train_l, db_train_u, db_val)\n",
    "\n",
    "        final_path = f\"{model_save_path}.num_labeled_{num_labeled}_fold_{fold}.final.pth\"\n",
    "        self_train(model, model, snapshot_path, final_path, db_train_l, db_train_u, db_val)\n",
    "\n",
    "    ## Test\n",
    "    print(\"Test Stage\")\n",
    "    data = []\n",
    "    label = []\n",
    "    preds = []\n",
    "    seg_metrics_macro = []\n",
    "    deli_metrics_macro = []\n",
    "    \n",
    "    for fold in range(5):\n",
    "        model = UNet1D_A(length=2500, base_channels=16, kernel_size=9, dropout='channels', droprate=.2, num_classes=2).to(device)\n",
    "        model_load_path = f\"{model_save_path}.num_labeled_{num_labeled}_fold_{fold}.final.pth\"\n",
    "        model.load_state_dict(torch.load(model_load_path))\n",
    "\n",
    "        x_train, y_train, _, x_test, y_test = raw_data_load_ludb(40, 160, fold, crop=[0, 5000])\n",
    "        test_dataset = ECGDataset(x_test, y_test, transform=base_transforms())\n",
    "\n",
    "        pred = model_predict(model, model_load_path, test_dataset, device, multi_lead_correction=False)\n",
    "        \n",
    "        flag_ludb = np.load('./dataset/ludb/flag.npy')\n",
    "        index_shuffled_5fold = np.load('./dataset/ludb/ludb_index_shuffled_5fold_250113.npy')\n",
    "        index_shuffled = index_shuffled_5fold[:,fold]\n",
    "        index_shuffled_lead = []\n",
    "        for i in np.array(index_shuffled):\n",
    "            index_shuffled_lead.extend([k for k in range(12*i,12*i+12,1)])\n",
    "        num_test = 40\n",
    "        flag_test= flag_ludb[index_shuffled_lead[0:num_test*12]]\n",
    "        dataset = (x_test, y_test, np.zeros((x_test.shape[0],)), flag_test)\n",
    "        _, _, seg_metrics, deli_metrics = dataset_eval(dataset, pred, th_delineation=th_delineation, verbose=0)\n",
    "\n",
    "        data.append(x_test)\n",
    "        label.append(y_test)\n",
    "        preds.append(pred)\n",
    "        seg_metrics_macro.append(seg_metrics)\n",
    "        deli_metrics_macro.append(deli_metrics)\n",
    "\n",
    "    data = np.concatenate(data, axis=0)\n",
    "    label = np.concatenate(label, axis=0)\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "\n",
    "    def summarize_total_rows(dfs):\n",
    "        \"\"\"\n",
    "        Extracts 'Total' rows from DataFrames, calculates summary statistics,\n",
    "        and returns a new DataFrame.\n",
    "\n",
    "        Args:\n",
    "        dfs: A list of pandas DataFrames with identical structure.\n",
    "\n",
    "        Returns:\n",
    "            A pandas DataFrame containing the mean, std, max, and min\n",
    "            of each column of the 'Total' rows from all input DataFrames.\n",
    "        \"\"\"\n",
    "        total_rows = [df[df['type'] == 'Total'].iloc[0] for df in dfs]\n",
    "        total_df = pd.DataFrame(total_rows)\n",
    "\n",
    "        # Get original headers, and remove 'type'\n",
    "        original_headers = total_df.columns.tolist()\n",
    "        original_headers.remove('type')\n",
    "\n",
    "\n",
    "        # Calculate summary stats for each column (excluding type)\n",
    "        summary_data = {\n",
    "            'mean': total_df[original_headers].mean().to_list(),\n",
    "            'std': total_df[original_headers].std().to_list(),\n",
    "            'max': total_df[original_headers].max().to_list(),\n",
    "            'min': total_df[original_headers].min().to_list()\n",
    "        }\n",
    "        # Create the summary DataFrame\n",
    "        summary_df = pd.DataFrame(summary_data, index = original_headers)\n",
    "        return summary_df\n",
    "    \n",
    "    # Macro average metrics of 5 folds\n",
    "    seg_metrics_macro = summarize_total_rows(seg_metrics_macro)\n",
    "    deli_metrics_macro = summarize_total_rows(deli_metrics_macro) \n",
    "    filtered_df = deli_metrics_macro[deli_metrics_macro.index.str.contains('f1')]\n",
    "    merged_df_macro = pd.concat([seg_metrics_macro, filtered_df], axis = 0)                \n",
    "\n",
    "    # Micro average metrics of 5 folds\n",
    "    dataset = (data, label, np.zeros((data.shape[0],)), np.zeros((data.shape[0],)))\n",
    "    _, _, seg_metrics_micro, deli_metrics_micro = dataset_eval(dataset, preds, th_delineation=th_delineation, verbose=0)\n",
    "    # Filter df2 to include rows where column name contain 'f1'\n",
    "    filtered_df = deli_metrics_micro[['type'] + [col for col in deli_metrics_micro.columns if 'f1' in col]]\n",
    "    merged_df_micro = pd.merge(seg_metrics_micro, filtered_df, on='type', how='outer')\n",
    "    micro_row = merged_df_micro[merged_df_micro['type'] == 'Total'].iloc[0]\n",
    "    # Remove 'type' and convert to series\n",
    "    micro_row_values = micro_row.drop('type')\n",
    "    merged_df = copy.deepcopy(merged_df_macro)\n",
    "    merged_df['micro'] = micro_row_values\n",
    "    \n",
    "    df_list.append(merged_df)\n",
    "    # Final results\n",
    "    print(merged_df)\n",
    "\n",
    "\n",
    "# Save results\n",
    "# 拼接数据，将总标题作为列上方的“标题行”\n",
    "concat_frames = []\n",
    "for i, df in enumerate(df_list):\n",
    "    # 插入标题行\n",
    "    df_with_title = df.copy()\n",
    "    df_with_title.columns = pd.MultiIndex.from_tuples([(str(records_list[i]), col) for col in df.columns])\n",
    "    concat_frames.append(df_with_title)\n",
    "\n",
    "# 按列拼接，并保留行标题\n",
    "result = pd.concat(concat_frames, axis=1)\n",
    "\n",
    "# 写入 Excel\n",
    "result.to_excel(f\"{metrics_save_path}.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
