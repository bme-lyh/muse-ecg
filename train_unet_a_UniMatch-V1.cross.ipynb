{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, LambdaLR\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from network.model_unet_a_2d import *\n",
    "from loss_utils import *\n",
    "from data_loader import *\n",
    "from data_augmentation import *\n",
    "from test_utils import model_predict, dataset_eval\n",
    "from torchinfo import summary\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import math\n",
    "from data_augmentation import *\n",
    "\n",
    "\n",
    "def obtain_cutmix_box(signal_size, p=0.5, length_min=0.02, length_max=0.4):\n",
    "    # 初始化全 0 的 1D 掩码\n",
    "    mask = np.zeros((signal_size, 1))\n",
    "    \n",
    "    # 以概率 p 决定是否应用 CutMix\n",
    "    if random.random() > p:\n",
    "        return mask\n",
    "\n",
    "    # 随机选择裁剪长度，范围为 length_min * signal_size 到 length_max * signal_size\n",
    "    cutmix_len = int(np.random.uniform(length_min, length_max) * signal_size)\n",
    "    \n",
    "    # 随机选择起始位置，确保裁剪区域在信号内\n",
    "    start = np.random.randint(0, signal_size - cutmix_len + 1)\n",
    "    \n",
    "    # 将掩码中对应区域设为 1\n",
    "    mask[start:start + cutmix_len, :] = 1\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "class SemiDataset(Dataset):\n",
    "    def __init__(self, x, y, mode, nsample=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.mode = mode\n",
    "        self.ids = list(range(len(x)))\n",
    "        if mode == 'train_l' and nsample is not None:\n",
    "            self.ids *= math.ceil(nsample / len(self.ids))\n",
    "            self.ids = self.ids[:nsample]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        ecg = self.x[item]\n",
    "        mask = self.y[item]\n",
    "\n",
    "        if self.mode == 'val':\n",
    "            ecg = zscore_normalize(ecg, axis=0)\n",
    "            ecg = torch.from_numpy(ecg.astype(np.float32)).permute(1, 0).unsqueeze(1) # (channel=1, 1, length)\n",
    "            mask = torch.from_numpy(mask.astype(np.float32)).permute(1, 0).unsqueeze(1) # (channel=4, 1, length)\n",
    "            return ecg, mask\n",
    "\n",
    "        ecg, mask = random_resize(ecg, mask, scale_range=(0.5, 2))\n",
    "        if random.random() < 0.5:\n",
    "            ecg, mask = np.flip(ecg, axis=0).copy(), np.flip(mask, axis=0).copy()\n",
    "\n",
    "        if self.mode == 'train_l':\n",
    "            ecg = zscore_normalize(ecg, axis=0)\n",
    "            ecg = torch.from_numpy(ecg.astype(np.float32)).permute(1, 0).unsqueeze(1)\n",
    "            mask = torch.from_numpy(mask.astype(np.float32)).permute(1, 0).unsqueeze(1)\n",
    "            return ecg, mask\n",
    "\n",
    "        ecg_w, ecg_s1, ecg_s2 = deepcopy(ecg), deepcopy(ecg), deepcopy(ecg)\n",
    "\n",
    "        if random.random() < 0.8:\n",
    "            ecg_s1 = ecg_s1 + baseline_wander_noise(ecg_s1[:,0], fs=500, snr=-10, freq=0.15)[:,np.newaxis]\n",
    "        if random.random() < 0.5:\n",
    "            ecg_s1 = ecg_s1 + additive_white_gaussian_noise(ecg_s1[:,0], snr=10)[:,np.newaxis]\n",
    "        cutmix_box1 = obtain_cutmix_box(ecg_s1.shape[0], p=0.5)\n",
    "\n",
    "        if random.random() < 0.8:\n",
    "            ecg_s2 = ecg_s2 + baseline_wander_noise(ecg_s2[:,0], fs=500, snr=-10, freq=0.15)[:,np.newaxis]\n",
    "        if random.random() < 0.5:\n",
    "            ecg_s2 = ecg_s2 + additive_white_gaussian_noise(ecg_s2[:,0], snr=10)[:,np.newaxis]\n",
    "        cutmix_box2 = obtain_cutmix_box(ecg_s2.shape[0], p=0.5)\n",
    "\n",
    "        ecg_w = zscore_normalize(ecg_w, axis=0)\n",
    "        ecg_s1 = zscore_normalize(ecg_s1, axis=0)\n",
    "        ecg_s2 = zscore_normalize(ecg_s2, axis=0)\n",
    "\n",
    "        ecg_w = torch.from_numpy(ecg_w.astype(np.float32)).permute(1, 0).unsqueeze(1)\n",
    "        ecg_s1 = torch.from_numpy(ecg_s1.astype(np.float32)).permute(1, 0).unsqueeze(1)\n",
    "        ecg_s2 = torch.from_numpy(ecg_s2.astype(np.float32)).permute(1, 0).unsqueeze(1)\n",
    "        mask = torch.from_numpy(mask.astype(np.float32)).permute(1, 0).unsqueeze(1)\n",
    "        cutmix_box1 = torch.from_numpy(cutmix_box1.astype(np.float32)).permute(1, 0).unsqueeze(1)\n",
    "        cutmix_box2 = torch.from_numpy(cutmix_box2.astype(np.float32)).permute(1, 0).unsqueeze(1)\n",
    "        \n",
    "        ignore_mask = torch.zeros((mask.shape[1], mask.shape[2]), dtype=torch.float32)\n",
    "        return ecg_w, ecg_s1, ecg_s2, ignore_mask, cutmix_box1, cutmix_box2\n",
    "    \n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, length=0):\n",
    "        self.length = length\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        if self.length > 0:\n",
    "            self.history = []\n",
    "        else:\n",
    "            self.count = 0\n",
    "            self.sum = 0.0\n",
    "        self.val = 0.0\n",
    "        self.avg = 0.0\n",
    "\n",
    "    def update(self, val, num=1):\n",
    "        if self.length > 0:\n",
    "            # currently assert num==1 to avoid bad usage, refine when there are some explict requirements\n",
    "            assert num == 1\n",
    "            self.history.append(val)\n",
    "            if len(self.history) > self.length:\n",
    "                del self.history[0]\n",
    "\n",
    "            self.val = self.history[-1]\n",
    "            self.avg = np.mean(self.history)\n",
    "        else:\n",
    "            self.val = val\n",
    "            self.sum += val * num\n",
    "            self.count += num\n",
    "            self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def train_Unimatch(model, trainset_u, trainset_l, valset, model_path, deep_supervision=False):\n",
    "    batch_size = 32\n",
    "    ini_lr = 1e-1\n",
    "    lr = copy.deepcopy(ini_lr)\n",
    "    epochs = 80\n",
    "    # epochs = 1\n",
    "    conf_thresh = 0.95\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n",
    "    criterion_l = nn.CrossEntropyLoss()\n",
    "    criterion_u = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    trainloader_u = DataLoader(trainset_u, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    trainloader_l = DataLoader(trainset_l, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    valloader = DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    total_iters = len(trainloader_u) * epochs\n",
    "    best_val_loss = np.inf\n",
    "    with tqdm(total=epochs, desc='Training Progress', unit='epoch') as pbar:\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = AverageMeter()\n",
    "            total_loss_x = AverageMeter()\n",
    "            total_loss_s = AverageMeter()\n",
    "            total_loss_w_fp = AverageMeter()\n",
    "            total_mask_ratio = AverageMeter()\n",
    "\n",
    "            loader = zip(trainloader_l, trainloader_u, trainloader_u)\n",
    "            for i, ((ecg_x, mask_x),\n",
    "                    (ecg_u_w, ecg_u_s1, ecg_u_s2, ignore_mask, cutmix_box1, cutmix_box2),\n",
    "                    (ecg_u_w_mix, ecg_u_s1_mix, ecg_u_s2_mix, ignore_mask_mix, _, _)) in enumerate(loader):\n",
    "                \n",
    "                ecg_x, mask_x = ecg_x.cuda(), mask_x.cuda()\n",
    "                ecg_u_w = ecg_u_w.cuda()\n",
    "                ecg_u_s1, ecg_u_s2, ignore_mask = ecg_u_s1.cuda(), ecg_u_s2.cuda(), ignore_mask.cuda()\n",
    "                cutmix_box1, cutmix_box2 = cutmix_box1.cuda(), cutmix_box2.cuda()\n",
    "                ecg_u_w_mix = ecg_u_w_mix.cuda()\n",
    "                ecg_u_s1_mix, ecg_u_s2_mix = ecg_u_s1_mix.cuda(), ecg_u_s2_mix.cuda()\n",
    "                ignore_mask_mix = ignore_mask_mix.cuda()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "\n",
    "                    pred_u_w_mix = model(ecg_u_w_mix).detach()\n",
    "                    conf_u_w_mix = pred_u_w_mix.max(dim=1)[0]\n",
    "                    mask_u_w_mix = pred_u_w_mix.argmax(dim=1)\n",
    "\n",
    "                ecg_u_s1[cutmix_box1 == 1] = \\\n",
    "                    ecg_u_s1_mix[cutmix_box1 == 1]\n",
    "                ecg_u_s2[cutmix_box2 == 1] = \\\n",
    "                    ecg_u_s2_mix[cutmix_box2 == 1]\n",
    "\n",
    "                model.train()\n",
    "\n",
    "                num_lb, num_ulb = ecg_x.shape[0], ecg_u_w.shape[0]\n",
    "\n",
    "                preds, preds_fp = model(torch.cat((ecg_x, ecg_u_w)), True)\n",
    "                pred_x, pred_u_w = preds.split([num_lb, num_ulb])\n",
    "                pred_u_w_fp = preds_fp[num_lb:]\n",
    "\n",
    "                pred_u_s1, pred_u_s2 = model(torch.cat((ecg_u_s1, ecg_u_s2))).chunk(2)\n",
    "\n",
    "                pred_u_w = pred_u_w.detach()\n",
    "                conf_u_w = pred_u_w.max(dim=1)[0]\n",
    "                mask_u_w = pred_u_w.argmax(dim=1)\n",
    "\n",
    "                mask_u_w_cutmixed1, conf_u_w_cutmixed1, ignore_mask_cutmixed1 = \\\n",
    "                    mask_u_w.clone(), conf_u_w.clone(), ignore_mask.clone()\n",
    "                mask_u_w_cutmixed2, conf_u_w_cutmixed2, ignore_mask_cutmixed2 = \\\n",
    "                    mask_u_w.clone(), conf_u_w.clone(), ignore_mask.clone()\n",
    "\n",
    "                mask_u_w_cutmixed1[cutmix_box1.squeeze(1) == 1] = mask_u_w_mix[cutmix_box1.squeeze(1) == 1]\n",
    "                conf_u_w_cutmixed1[cutmix_box1.squeeze(1) == 1] = conf_u_w_mix[cutmix_box1.squeeze(1) == 1]\n",
    "                ignore_mask_cutmixed1[cutmix_box1.squeeze(1) == 1] = ignore_mask_mix[cutmix_box1.squeeze(1) == 1]\n",
    "\n",
    "                mask_u_w_cutmixed2[cutmix_box2.squeeze(1) == 1] = mask_u_w_mix[cutmix_box2.squeeze(1) == 1]\n",
    "                conf_u_w_cutmixed2[cutmix_box2.squeeze(1) == 1] = conf_u_w_mix[cutmix_box2.squeeze(1) == 1]\n",
    "                ignore_mask_cutmixed2[cutmix_box2.squeeze(1) == 1] = ignore_mask_mix[cutmix_box2.squeeze(1) == 1]\n",
    "                \n",
    "                if deep_supervision:\n",
    "                    pred_xs = model(ecg_x, full_output=True)[0:4]\n",
    "                    loss_x = sum([criterion_l(pred, mask_x) for pred in pred_xs])\n",
    "                else:\n",
    "                    loss_x = criterion_l(pred_x, mask_x)\n",
    "\n",
    "                loss_u_s1 = criterion_u(pred_u_s1, mask_u_w_cutmixed1)\n",
    "                loss_u_s1 = loss_u_s1 * ((conf_u_w_cutmixed1 >= conf_thresh) & (ignore_mask_cutmixed1 != 255))\n",
    "                loss_u_s1 = loss_u_s1.sum() / (ignore_mask_cutmixed1 != 255).sum().item()\n",
    "\n",
    "                loss_u_s2 = criterion_u(pred_u_s2, mask_u_w_cutmixed2)\n",
    "                loss_u_s2 = loss_u_s2 * ((conf_u_w_cutmixed2 >= conf_thresh) & (ignore_mask_cutmixed2 != 255))\n",
    "                loss_u_s2 = loss_u_s2.sum() / (ignore_mask_cutmixed2 != 255).sum().item()\n",
    "\n",
    "                loss_u_w_fp = criterion_u(pred_u_w_fp, mask_u_w)\n",
    "                loss_u_w_fp = loss_u_w_fp * ((conf_u_w >= conf_thresh) & (ignore_mask != 255))\n",
    "                loss_u_w_fp = loss_u_w_fp.sum() / (ignore_mask != 255).sum().item()\n",
    "\n",
    "                loss = (loss_x + loss_u_s1 * 0.25 + loss_u_s2 * 0.25 + loss_u_w_fp * 0.5) / 2.0\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss.update(loss.item())\n",
    "                total_loss_x.update(loss_x.item())\n",
    "                total_loss_s.update((loss_u_s1.item() + loss_u_s2.item()) / 2.0)\n",
    "                total_loss_w_fp.update(loss_u_w_fp.item())\n",
    "                \n",
    "                mask_ratio = ((conf_u_w >= conf_thresh) & (ignore_mask != 255)).sum().item() / \\\n",
    "                    (ignore_mask != 255).sum()\n",
    "                total_mask_ratio.update(mask_ratio.item())\n",
    "\n",
    "                iters = epoch * len(trainloader_u) + i\n",
    "                lr = ini_lr * (1 - iters / total_iters) ** 0.9\n",
    "                optimizer.param_groups[0][\"lr\"] = lr\n",
    "\n",
    "            # validation\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            for ecg, mask in valloader:\n",
    "                ecg, mask = ecg.cuda(), mask.cuda()\n",
    "                pred = model(ecg)\n",
    "                loss = criterion_l(pred, mask)\n",
    "                val_loss += loss.item()\n",
    "            val_loss /= len(valloader)\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), f'{model_path}.best.pth')\n",
    "\n",
    "            pbar.set_postfix_str(f'loss: {total_loss.avg:.4f}, val_loss: {val_loss:.4f}, mask_ratio: {total_mask_ratio.avg:.4f}')\n",
    "            pbar.update(1)\n",
    "\n",
    "        torch.save(model.state_dict(), f'{model_path}.final.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labeled data: 5\n",
      "Training Stage\n",
      "Test Stage\n",
      "                mean       std       max       min     micro\n",
      "iou_p       0.625751  0.078055  0.708601  0.527379  0.624759\n",
      "iou_qrs     0.842977  0.012109  0.857707  0.832309  0.840664\n",
      "iou_t       0.749867  0.036341  0.798074  0.711801  0.750223\n",
      "miou        0.739531  0.040579  0.788127  0.697687  0.738549\n",
      "acc         0.891814  0.011076  0.907942  0.882268  0.890136\n",
      "ave_f1      0.925151  0.024597  0.954095  0.901746  0.924202\n",
      "f1_p_on     0.878912  0.039252  0.932827  0.832110  0.875951\n",
      "f1_p_end    0.879126  0.039217  0.933739  0.832110  0.876129\n",
      "f1_qrs_on   0.987336  0.007244  0.996551  0.979779  0.987479\n",
      "f1_qrs_end  0.986742  0.008285  0.996551  0.977523   0.98653\n",
      "f1_t_on     0.909607  0.033671  0.946257  0.861512  0.909732\n",
      "f1_t_end    0.909182  0.031719  0.943831  0.864204  0.909391\n",
      "Number of labeled data: 10\n",
      "Training Stage\n",
      "Test Stage\n",
      "                mean       std       max       min     micro\n",
      "iou_p       0.722013  0.024523  0.758033  0.693607  0.716935\n",
      "iou_qrs     0.858398  0.008573  0.870331  0.846409  0.856156\n",
      "iou_t       0.793346  0.015983  0.818019  0.776987  0.793175\n",
      "miou        0.791253  0.009321  0.799754  0.776709  0.788756\n",
      "acc         0.909616  0.004680  0.917265  0.906401  0.908498\n",
      "ave_f1      0.950720  0.011193  0.966970  0.938606  0.949433\n",
      "f1_p_on     0.917515  0.024260  0.946835  0.892207  0.913301\n",
      "f1_p_end    0.918047  0.024633  0.947414  0.890963  0.913831\n",
      "f1_qrs_on   0.992127  0.002607  0.995553  0.989062  0.992101\n",
      "f1_qrs_end  0.991982  0.002628  0.995553  0.988817  0.991602\n",
      "f1_t_on     0.942509  0.013788  0.961885  0.924723  0.943129\n",
      "f1_t_end    0.942138  0.013489  0.960683  0.924465  0.942632\n",
      "Number of labeled data: 20\n",
      "Training Stage\n",
      "Test Stage\n",
      "                mean       std       max       min     micro\n",
      "iou_p       0.768515  0.030750  0.798260  0.726722  0.763725\n",
      "iou_qrs     0.869711  0.013034  0.887633  0.852962  0.868326\n",
      "iou_t       0.828341  0.015661  0.855782  0.817194  0.828337\n",
      "miou        0.822189  0.016584  0.846165  0.804858  0.820129\n",
      "acc         0.921476  0.006230  0.932184  0.916350  0.920639\n",
      "ave_f1      0.969317  0.009317  0.980789  0.959706  0.968276\n",
      "f1_p_on     0.945291  0.016209  0.964932  0.925035   0.94172\n",
      "f1_p_end    0.946066  0.016325  0.965795  0.925664   0.94249\n",
      "f1_qrs_on   0.993562  0.002414  0.996313  0.990182  0.993529\n",
      "f1_qrs_end  0.993515  0.002411  0.996313  0.990182   0.99312\n",
      "f1_t_on     0.969154  0.010230  0.981677  0.960191  0.969801\n",
      "f1_t_end    0.968313  0.010584  0.981920  0.958372  0.968994\n",
      "Number of labeled data: 50\n",
      "Training Stage\n",
      "Test Stage\n",
      "                mean       std       max       min     micro\n",
      "iou_p       0.800801  0.033316  0.831050  0.748553  0.794745\n",
      "iou_qrs     0.887090  0.007586  0.895431  0.878421  0.885316\n",
      "iou_t       0.848828  0.018026  0.871210  0.830521  0.849089\n",
      "miou        0.845573  0.017121  0.865897  0.821458   0.84305\n",
      "acc         0.931529  0.007221  0.940904  0.924179  0.930613\n",
      "ave_f1      0.977995  0.007401  0.983639  0.965030  0.976648\n",
      "f1_p_on     0.957381  0.015445  0.972323  0.932688    0.9529\n",
      "f1_p_end    0.957381  0.015802  0.972323  0.932063    0.9529\n",
      "f1_qrs_on   0.996217  0.001572  0.997997  0.993921  0.996277\n",
      "f1_qrs_end  0.996125  0.001743  0.997997  0.993463  0.995732\n",
      "f1_t_on     0.981051  0.008082  0.987045  0.968228  0.981647\n",
      "f1_t_end    0.979816  0.008232  0.985847  0.966311  0.980434\n",
      "Number of labeled data: 160\n",
      "Training Stage\n",
      "Fold 1/5: Train labeled: 1536, Train unlabeled: 23970, Val: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 80/80 [1:41:27<00:00, 76.10s/epoch, loss: 1.9688, val_loss: 0.7973, mask_ratio: 0.9628]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5: Train labeled: 1536, Train unlabeled: 23970, Val: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 80/80 [1:41:10<00:00, 75.88s/epoch, loss: 1.9669, val_loss: 0.8087, mask_ratio: 0.9643]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5: Train labeled: 1536, Train unlabeled: 23970, Val: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 80/80 [1:41:15<00:00, 75.94s/epoch, loss: 1.9715, val_loss: 0.7997, mask_ratio: 0.9631]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5: Train labeled: 1536, Train unlabeled: 23970, Val: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 80/80 [1:41:06<00:00, 75.83s/epoch, loss: 1.9682, val_loss: 0.7964, mask_ratio: 0.9642]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5: Train labeled: 1536, Train unlabeled: 23970, Val: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 80/80 [1:41:08<00:00, 75.86s/epoch, loss: 1.9666, val_loss: 0.8012, mask_ratio: 0.9636]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Stage\n",
      "                mean       std       max       min     micro\n",
      "iou_p       0.830528  0.023219  0.857632  0.793596  0.826733\n",
      "iou_qrs     0.897515  0.009313  0.911197  0.886583   0.89626\n",
      "iou_t       0.863331  0.018941  0.893228  0.845948  0.863583\n",
      "miou        0.863791  0.015953  0.887352  0.843737  0.862192\n",
      "acc         0.939568  0.006736  0.950891  0.933983  0.938974\n",
      "ave_f1      0.984933  0.005411  0.992721  0.978894  0.984175\n",
      "f1_p_on     0.972682  0.009268  0.986264  0.960600  0.970039\n",
      "f1_p_end    0.972927  0.009075  0.985962  0.960600  0.970277\n",
      "f1_qrs_on   0.996359  0.001842  0.998219  0.993599  0.996369\n",
      "f1_qrs_end  0.996357  0.001912  0.998219  0.993599  0.995916\n",
      "f1_t_on     0.985828  0.007594  0.994078  0.976924  0.986427\n",
      "f1_t_end    0.985442  0.007429  0.994851  0.976924  0.986023\n"
     ]
    }
   ],
   "source": [
    "th_delineation = 150\n",
    "gpu = 0\n",
    "aug = 2\n",
    "deep_supervision = 1\n",
    "torch.cuda.set_device(gpu)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Parameters\n",
    "model_save_path = f'./checkpoints/unet_a_ds{int(deep_supervision)}_UniMatch-V1.cross'\n",
    "metrics_save_path = f'./metrics/unet_a_ds{int(deep_supervision)}_UniMatch-V1.cross'\n",
    "records_list = [5,10,20,50,160]\n",
    "val_ratio = 0.2\n",
    "df_list = []\n",
    "\n",
    "for num_labeled in records_list:\n",
    "    ## Train\n",
    "    print(f\"Number of labeled data: {num_labeled}\")\n",
    "    print(\"Training Stage\")\n",
    "    for fold in range(5):\n",
    "        if os.path.exists(f\"{model_save_path}.num_labeled_{num_labeled}_fold_{fold}.final.pth\"):\n",
    "            continue\n",
    "        # Set random seed for reproducibility\n",
    "        seed = 42\n",
    "        cudnn.benchmark = False\n",
    "        cudnn.deterministic = True\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "        x_train_l, y_train_l, _, _, _ = raw_data_load_ludb(40, num_labeled, fold, crop=[1250, 3750])\n",
    "        x_train_u, y_train_u, _, _, _ = raw_data_load_rdb(400, 1999, fold, crop=[1250, 3750])\n",
    "\n",
    "        num_val =  np.round(x_train_l.shape[0] * val_ratio).astype(int)\n",
    "        x_val, y_val = x_train_l[:num_val], y_train_l[:num_val]\n",
    "        x_train_l, y_train_l = x_train_l[num_val:], y_train_l[num_val:]\n",
    "\n",
    "        print(f\"Fold {fold+1}/{5}: Train labeled: {x_train_l.shape[0]}, Train unlabeled: {x_train_u.shape[0]}, Val: {x_val.shape[0]}\")\n",
    "        \n",
    "        model = UNet1D_A(length=2500, base_channels=16, kernel_size=9, dropout='channels', droprate=.2, num_classes=2).to('cuda')\n",
    "        ini_ds = deep_supervision\n",
    "        ini_aug = 2\n",
    "        model_load_path = f\"./checkpoints/unet_a_ds{ini_ds}.num_labeled_{num_labeled}_aug_{ini_aug}.fold_{fold}.epoch_20.pth\"\n",
    "        model.load_state_dict(torch.load(model_load_path))\n",
    "        trainset_u = SemiDataset(x_train_u, y_train_u, 'train_u')\n",
    "        trainset_l = SemiDataset(x_train_l, y_train_l, 'train_l')\n",
    "        valset = SemiDataset(x_val, y_val, 'val')\n",
    "\n",
    "        model_path = f\"{model_save_path}.num_labeled_{num_labeled}_fold_{fold}\"\n",
    "        train_Unimatch(model, trainset_u, trainset_l, valset, model_path, deep_supervision=deep_supervision)\n",
    "\n",
    "    ## Test\n",
    "    print(\"Test Stage\")\n",
    "    data = []\n",
    "    label = []\n",
    "    preds = []\n",
    "    seg_metrics_macro = []\n",
    "    deli_metrics_macro = []\n",
    "    \n",
    "    for fold in range(5):\n",
    "        model = UNet1D_A(length=2500, base_channels=16, kernel_size=9, dropout='channels', droprate=.2, num_classes=2).to(device)\n",
    "        model_load_path = f\"{model_save_path}.num_labeled_{num_labeled}_fold_{fold}.best.pth\"\n",
    "        model.load_state_dict(torch.load(model_load_path))\n",
    "\n",
    "        x_train, y_train, _, x_test, y_test = raw_data_load_ludb(40, 160, fold, crop=[0, 5000])\n",
    "        test_dataset = ECGDataset(x_test, y_test, transform=base_transforms())\n",
    "\n",
    "        pred = model_predict(model, model_load_path, test_dataset, device, multi_lead_correction=False)\n",
    "        \n",
    "        flag_ludb = np.load('./dataset/ludb/flag.npy')\n",
    "        index_shuffled_5fold = np.load('./dataset/ludb/ludb_index_shuffled_5fold_250113.npy')\n",
    "        index_shuffled = index_shuffled_5fold[:,fold]\n",
    "        index_shuffled_lead = []\n",
    "        for i in np.array(index_shuffled):\n",
    "            index_shuffled_lead.extend([k for k in range(12*i,12*i+12,1)])\n",
    "        num_test = 40\n",
    "        flag_test= flag_ludb[index_shuffled_lead[0:num_test*12]]\n",
    "        dataset = (x_test, y_test, np.zeros((x_test.shape[0],)), flag_test)\n",
    "        _, _, seg_metrics, deli_metrics = dataset_eval(dataset, pred, th_delineation=th_delineation, verbose=0)\n",
    "\n",
    "        data.append(x_test)\n",
    "        label.append(y_test)\n",
    "        preds.append(pred)\n",
    "        seg_metrics_macro.append(seg_metrics)\n",
    "        deli_metrics_macro.append(deli_metrics)\n",
    "\n",
    "    data = np.concatenate(data, axis=0)\n",
    "    label = np.concatenate(label, axis=0)\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "\n",
    "    def summarize_total_rows(dfs):\n",
    "        \"\"\"\n",
    "        Extracts 'Total' rows from DataFrames, calculates summary statistics,\n",
    "        and returns a new DataFrame.\n",
    "\n",
    "        Args:\n",
    "        dfs: A list of pandas DataFrames with identical structure.\n",
    "\n",
    "        Returns:\n",
    "            A pandas DataFrame containing the mean, std, max, and min\n",
    "            of each column of the 'Total' rows from all input DataFrames.\n",
    "        \"\"\"\n",
    "        total_rows = [df[df['type'] == 'Total'].iloc[0] for df in dfs]\n",
    "        total_df = pd.DataFrame(total_rows)\n",
    "\n",
    "        # Get original headers, and remove 'type'\n",
    "        original_headers = total_df.columns.tolist()\n",
    "        original_headers.remove('type')\n",
    "\n",
    "\n",
    "        # Calculate summary stats for each column (excluding type)\n",
    "        summary_data = {\n",
    "            'mean': total_df[original_headers].mean().to_list(),\n",
    "            'std': total_df[original_headers].std().to_list(),\n",
    "            'max': total_df[original_headers].max().to_list(),\n",
    "            'min': total_df[original_headers].min().to_list()\n",
    "        }\n",
    "        # Create the summary DataFrame\n",
    "        summary_df = pd.DataFrame(summary_data, index = original_headers)\n",
    "        return summary_df\n",
    "    \n",
    "    # Macro average metrics of 5 folds\n",
    "    seg_metrics_macro = summarize_total_rows(seg_metrics_macro)\n",
    "    deli_metrics_macro = summarize_total_rows(deli_metrics_macro) \n",
    "    filtered_df = deli_metrics_macro[deli_metrics_macro.index.str.contains('f1')]\n",
    "    merged_df_macro = pd.concat([seg_metrics_macro, filtered_df], axis = 0)                \n",
    "\n",
    "    # Micro average metrics of 5 folds\n",
    "    dataset = (data, label, np.zeros((data.shape[0],)), np.zeros((data.shape[0],)))\n",
    "    _, _, seg_metrics_micro, deli_metrics_micro = dataset_eval(dataset, preds, th_delineation=th_delineation, verbose=0)\n",
    "    # Filter df2 to include rows where column name contain 'f1'\n",
    "    filtered_df = deli_metrics_micro[['type'] + [col for col in deli_metrics_micro.columns if 'f1' in col]]\n",
    "    merged_df_micro = pd.merge(seg_metrics_micro, filtered_df, on='type', how='outer')\n",
    "    micro_row = merged_df_micro[merged_df_micro['type'] == 'Total'].iloc[0]\n",
    "    # Remove 'type' and convert to series\n",
    "    micro_row_values = micro_row.drop('type')\n",
    "    merged_df = copy.deepcopy(merged_df_macro)\n",
    "    merged_df['micro'] = micro_row_values\n",
    "    \n",
    "    df_list.append(merged_df)\n",
    "    # Final results\n",
    "    print(merged_df)\n",
    "\n",
    "\n",
    "# Save results\n",
    "# 拼接数据，将总标题作为列上方的“标题行”\n",
    "concat_frames = []\n",
    "for i, df in enumerate(df_list):\n",
    "    # 插入标题行\n",
    "    df_with_title = df.copy()\n",
    "    df_with_title.columns = pd.MultiIndex.from_tuples([(str(records_list[i]), col) for col in df.columns])\n",
    "    concat_frames.append(df_with_title)\n",
    "\n",
    "# 按列拼接，并保留行标题\n",
    "result = pd.concat(concat_frames, axis=1)\n",
    "\n",
    "# 写入 Excel\n",
    "result.to_excel(f\"{metrics_save_path}.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
