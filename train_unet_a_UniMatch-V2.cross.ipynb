{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, LambdaLR\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from network.model_unet_a_2d import *\n",
    "from loss_utils import *\n",
    "from data_loader import *\n",
    "from data_augmentation import *\n",
    "from test_utils import model_predict, dataset_eval\n",
    "from torchinfo import summary\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import math\n",
    "from data_augmentation import *\n",
    "\n",
    "\n",
    "def obtain_cutmix_box(signal_size, p=0.5, length_min=0.02, length_max=0.4):\n",
    "    # 初始化全 0 的 1D 掩码\n",
    "    mask = np.zeros((signal_size, 1))\n",
    "    \n",
    "    # 以概率 p 决定是否应用 CutMix\n",
    "    if random.random() > p:\n",
    "        return mask\n",
    "\n",
    "    # 随机选择裁剪长度，范围为 length_min * signal_size 到 length_max * signal_size\n",
    "    cutmix_len = int(np.random.uniform(length_min, length_max) * signal_size)\n",
    "    \n",
    "    # 随机选择起始位置，确保裁剪区域在信号内\n",
    "    start = np.random.randint(0, signal_size - cutmix_len + 1)\n",
    "    \n",
    "    # 将掩码中对应区域设为 1\n",
    "    mask[start:start + cutmix_len, :] = 1\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "class SemiDataset(Dataset):\n",
    "    def __init__(self, x, y, mode, nsample=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.mode = mode\n",
    "        self.ids = list(range(len(x)))\n",
    "        if mode == 'train_l' and nsample is not None:\n",
    "            self.ids *= math.ceil(nsample / len(self.ids))\n",
    "            self.ids = self.ids[:nsample]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        ecg = self.x[item]\n",
    "        mask = self.y[item]\n",
    "\n",
    "        if self.mode == 'val':\n",
    "            ecg = zscore_normalize(ecg, axis=0)\n",
    "            ecg = torch.from_numpy(ecg.astype(np.float32)).permute(1, 0).unsqueeze(1) # (channel=1, 1, length)\n",
    "            mask = torch.from_numpy(mask.astype(np.float32)).permute(1, 0).unsqueeze(1) # (channel=4, 1, length)\n",
    "            return ecg, mask\n",
    "\n",
    "        ecg, mask = random_resize(ecg, mask, scale_range=(0.5, 2))\n",
    "        if random.random() < 0.5:\n",
    "            ecg, mask = np.flip(ecg, axis=0).copy(), np.flip(mask, axis=0).copy()\n",
    "\n",
    "        if self.mode == 'train_l':\n",
    "            ecg = zscore_normalize(ecg, axis=0)\n",
    "            ecg = torch.from_numpy(ecg.astype(np.float32)).permute(1, 0).unsqueeze(1)\n",
    "            mask = torch.from_numpy(mask.astype(np.float32)).permute(1, 0).unsqueeze(1)\n",
    "            return ecg, mask\n",
    "\n",
    "        ecg_w, ecg_s1, ecg_s2 = deepcopy(ecg), deepcopy(ecg), deepcopy(ecg)\n",
    "\n",
    "        if random.random() < 0.8:\n",
    "            ecg_s1 = ecg_s1 + baseline_wander_noise(ecg_s1[:,0], fs=500, snr=-10, freq=0.15)[:,np.newaxis]\n",
    "        if random.random() < 0.5:\n",
    "            ecg_s1 = ecg_s1 + additive_white_gaussian_noise(ecg_s1[:,0], snr=10)[:,np.newaxis]\n",
    "        cutmix_box1 = obtain_cutmix_box(ecg_s1.shape[0], p=0.5)\n",
    "\n",
    "        if random.random() < 0.8:\n",
    "            ecg_s2 = ecg_s2 + baseline_wander_noise(ecg_s2[:,0], fs=500, snr=-10, freq=0.15)[:,np.newaxis]\n",
    "        if random.random() < 0.5:\n",
    "            ecg_s2 = ecg_s2 + additive_white_gaussian_noise(ecg_s2[:,0], snr=10)[:,np.newaxis]\n",
    "        cutmix_box2 = obtain_cutmix_box(ecg_s2.shape[0], p=0.5)\n",
    "\n",
    "        ecg_w = zscore_normalize(ecg_w, axis=0)\n",
    "        ecg_s1 = zscore_normalize(ecg_s1, axis=0)\n",
    "        ecg_s2 = zscore_normalize(ecg_s2, axis=0)\n",
    "\n",
    "        ecg_w = torch.from_numpy(ecg_w.astype(np.float32)).permute(1, 0).unsqueeze(1)\n",
    "        ecg_s1 = torch.from_numpy(ecg_s1.astype(np.float32)).permute(1, 0).unsqueeze(1)\n",
    "        ecg_s2 = torch.from_numpy(ecg_s2.astype(np.float32)).permute(1, 0).unsqueeze(1)\n",
    "        mask = torch.from_numpy(mask.astype(np.float32)).permute(1, 0).unsqueeze(1)\n",
    "        cutmix_box1 = torch.from_numpy(cutmix_box1.astype(np.float32)).permute(1, 0).unsqueeze(1)\n",
    "        cutmix_box2 = torch.from_numpy(cutmix_box2.astype(np.float32)).permute(1, 0).unsqueeze(1)\n",
    "        \n",
    "        ignore_mask = torch.zeros((mask.shape[1], mask.shape[2]), dtype=torch.float32)\n",
    "        return ecg_w, ecg_s1, ecg_s2, ignore_mask, cutmix_box1, cutmix_box2\n",
    "    \n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, length=0):\n",
    "        self.length = length\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        if self.length > 0:\n",
    "            self.history = []\n",
    "        else:\n",
    "            self.count = 0\n",
    "            self.sum = 0.0\n",
    "        self.val = 0.0\n",
    "        self.avg = 0.0\n",
    "\n",
    "    def update(self, val, num=1):\n",
    "        if self.length > 0:\n",
    "            # currently assert num==1 to avoid bad usage, refine when there are some explict requirements\n",
    "            assert num == 1\n",
    "            self.history.append(val)\n",
    "            if len(self.history) > self.length:\n",
    "                del self.history[0]\n",
    "\n",
    "            self.val = self.history[-1]\n",
    "            self.avg = np.mean(self.history)\n",
    "        else:\n",
    "            self.val = val\n",
    "            self.sum += val * num\n",
    "            self.count += num\n",
    "            self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def train_Unimatch_V2(model, model_ema, trainset_u, trainset_l, valset, model_path, deep_supervision=False):\n",
    "    batch_size = 32\n",
    "    ini_lr = 1e-1\n",
    "    lr = copy.deepcopy(ini_lr)\n",
    "    epochs = 80\n",
    "    # epochs = 1\n",
    "    conf_thresh = 0.95\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n",
    "    criterion_l = nn.CrossEntropyLoss()\n",
    "    criterion_u = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    trainloader_u = DataLoader(trainset_u, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    trainloader_l = DataLoader(trainset_l, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    valloader = DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    total_iters = len(trainloader_u) * epochs\n",
    "    best_val_loss = np.inf\n",
    "    with tqdm(total=epochs, desc='Training Progress', unit='epoch') as pbar:\n",
    "        for epoch in range(epochs):\n",
    "            total_loss  = AverageMeter()\n",
    "            total_loss_x = AverageMeter()\n",
    "            total_loss_s = AverageMeter()\n",
    "            total_mask_ratio = AverageMeter()\n",
    "\n",
    "            loader = zip(trainloader_l, trainloader_u)\n",
    "            for i, ((ecg_x, mask_x),\n",
    "                    (ecg_u_w, ecg_u_s1, ecg_u_s2, ignore_mask, cutmix_box1, cutmix_box2)) in enumerate(loader):\n",
    "                \n",
    "                ecg_x, mask_x = ecg_x.cuda(), mask_x.cuda()\n",
    "                ecg_x, mask_x = ecg_x.cuda(), mask_x.cuda()\n",
    "                ecg_u_w, ecg_u_s1, ecg_u_s2 = ecg_u_w.cuda(), ecg_u_s1.cuda(), ecg_u_s2.cuda()\n",
    "                ignore_mask, cutmix_box1, cutmix_box2 = ignore_mask.cuda(), cutmix_box1.cuda(), cutmix_box2.cuda()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    pred_u_w = model_ema(ecg_u_w).detach()\n",
    "                    conf_u_w = pred_u_w.max(dim=1)[0]\n",
    "                    mask_u_w = pred_u_w.argmax(dim=1)\n",
    "                \n",
    "                ecg_u_s1[cutmix_box1 == 1] = ecg_u_s1.flip(0)[cutmix_box1 == 1]\n",
    "                ecg_u_s2[cutmix_box2 == 1] = ecg_u_s2.flip(0)[cutmix_box2 == 1]\n",
    "                \n",
    "                pred_x = model(ecg_x)\n",
    "                pred_u_s1, pred_u_s2 = model(torch.cat((ecg_u_s1, ecg_u_s2)), comp_drop=True).chunk(2)\n",
    "                \n",
    "                mask_u_w_cutmixed1, conf_u_w_cutmixed1, ignore_mask_cutmixed1 = mask_u_w.clone(), conf_u_w.clone(), ignore_mask.clone()\n",
    "                mask_u_w_cutmixed2, conf_u_w_cutmixed2, ignore_mask_cutmixed2 = mask_u_w.clone(), conf_u_w.clone(), ignore_mask.clone()\n",
    "\n",
    "                mask_u_w_cutmixed1[cutmix_box1.squeeze(1) == 1] = mask_u_w.flip(0)[cutmix_box1.squeeze(1) == 1]\n",
    "                conf_u_w_cutmixed1[cutmix_box1.squeeze(1) == 1] = conf_u_w.flip(0)[cutmix_box1.squeeze(1) == 1]\n",
    "                ignore_mask_cutmixed1[cutmix_box1.squeeze(1) == 1] = ignore_mask.flip(0)[cutmix_box1.squeeze(1) == 1]\n",
    "                \n",
    "                mask_u_w_cutmixed2[cutmix_box2.squeeze(1) == 1] = mask_u_w.flip(0)[cutmix_box2.squeeze(1) == 1]\n",
    "                conf_u_w_cutmixed2[cutmix_box2.squeeze(1) == 1] = conf_u_w.flip(0)[cutmix_box2.squeeze(1) == 1]\n",
    "                ignore_mask_cutmixed2[cutmix_box2.squeeze(1) == 1] = ignore_mask.flip(0)[cutmix_box2.squeeze(1) == 1]\n",
    "                \n",
    "                if deep_supervision:\n",
    "                    pred_xs = model(ecg_x, full_output=True)[0:4]\n",
    "                    loss_x = sum([criterion_l(pred, mask_x) for pred in pred_xs])\n",
    "                else:\n",
    "                    loss_x = criterion_l(pred_x, mask_x)\n",
    "\n",
    "                loss_u_s1 = criterion_u(pred_u_s1, mask_u_w_cutmixed1)\n",
    "                loss_u_s1 = loss_u_s1 * ((conf_u_w_cutmixed1 >= conf_thresh) & (ignore_mask_cutmixed1 != 255))\n",
    "                loss_u_s1 = loss_u_s1.sum() / (ignore_mask_cutmixed1 != 255).sum().item()\n",
    "                \n",
    "                loss_u_s2 = criterion_u(pred_u_s2, mask_u_w_cutmixed2)\n",
    "                loss_u_s2 = loss_u_s2 * ((conf_u_w_cutmixed2 >= conf_thresh) & (ignore_mask_cutmixed2 != 255))\n",
    "                loss_u_s2 = loss_u_s2.sum() / (ignore_mask_cutmixed2 != 255).sum().item()\n",
    "                \n",
    "                loss_u_s = (loss_u_s1 + loss_u_s2) / 2.0\n",
    "                \n",
    "                loss = (loss_x + loss_u_s) / 2.0\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss.update(loss.item())\n",
    "                total_loss_x.update(loss_x.item())\n",
    "                total_loss_s.update(loss_u_s.item())\n",
    "                mask_ratio = ((conf_u_w >= conf_thresh) & (ignore_mask != 255)).sum().item() / (ignore_mask != 255).sum()\n",
    "                total_mask_ratio.update(mask_ratio.item())\n",
    "\n",
    "                iters = epoch * len(trainloader_u) + i\n",
    "                lr = ini_lr * (1 - iters / total_iters) ** 0.9\n",
    "                optimizer.param_groups[0][\"lr\"] = lr\n",
    "\n",
    "                ema_ratio = min(1 - 1 / (iters + 1), 0.996)\n",
    "\n",
    "                for param, param_ema in zip(model.parameters(), model_ema.parameters()):\n",
    "                    param_ema.copy_(param_ema * ema_ratio + param.detach() * (1 - ema_ratio))\n",
    "                for buffer, buffer_ema in zip(model.buffers(), model_ema.buffers()):\n",
    "                    buffer_ema.copy_(buffer_ema * ema_ratio + buffer.detach() * (1 - ema_ratio))\n",
    "\n",
    "            # validation\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            for ecg, mask in valloader:\n",
    "                ecg, mask = ecg.cuda(), mask.cuda()\n",
    "                pred = model(ecg)\n",
    "                loss = criterion_l(pred, mask)\n",
    "                val_loss += loss.item()\n",
    "            val_loss /= len(valloader)\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), f'{model_path}.best.pth')\n",
    "\n",
    "            pbar.set_postfix_str(f'loss: {total_loss.avg:.4f}, val_loss: {val_loss:.4f}, mask_ratio: {total_mask_ratio.avg:.4f}')\n",
    "            pbar.update(1)\n",
    "\n",
    "        torch.save(model.state_dict(), f'{model_path}.final.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labeled data: 5\n",
      "Training Stage\n",
      "Test Stage\n",
      "                mean       std       max       min     micro\n",
      "iou_p       0.289564  0.288610  0.640228  0.000000   0.28951\n",
      "iou_qrs     0.789529  0.036395  0.825386  0.730953   0.78539\n",
      "iou_t       0.425166  0.352265  0.747151  0.000000  0.439723\n",
      "miou        0.501420  0.214588  0.737588  0.265016  0.504875\n",
      "acc         0.785052  0.099992  0.891327  0.663564  0.783775\n",
      "ave_f1      0.713442  0.250684  0.937403  0.318343  0.761703\n",
      "f1_p_on     0.574754  0.357865  0.906255  0.000001  0.629812\n",
      "f1_p_end    0.575255  0.358592  0.907261  0.000001  0.630502\n",
      "f1_qrs_on   0.968597  0.025272  0.993653  0.931144  0.967795\n",
      "f1_qrs_end  0.968784  0.025213  0.994114  0.931617  0.968063\n",
      "f1_t_on     0.596775  0.393428  0.921628  0.000001  0.687319\n",
      "f1_t_end    0.596489  0.393315  0.923721  0.000001  0.686727\n",
      "Number of labeled data: 10\n",
      "Training Stage\n",
      "Test Stage\n",
      "                mean       std       max       min     micro\n",
      "iou_p       0.704206  0.053158  0.769824  0.649230  0.697111\n",
      "iou_qrs     0.836995  0.026067  0.861853  0.807538  0.835515\n",
      "iou_t       0.790028  0.024516  0.809737  0.747821  0.790648\n",
      "miou        0.777076  0.032253  0.809504  0.734863  0.774425\n",
      "acc         0.898873  0.014791  0.909881  0.879724  0.899827\n",
      "ave_f1      0.956104  0.014867  0.974466  0.937804  0.954558\n",
      "f1_p_on     0.919937  0.027390  0.952299  0.886693   0.91477\n",
      "f1_p_end    0.920165  0.027408  0.952586  0.887022  0.915004\n",
      "f1_qrs_on   0.989997  0.006756  0.996404  0.980109  0.990158\n",
      "f1_qrs_end  0.989225  0.007252  0.996404  0.979411  0.989067\n",
      "f1_t_on     0.960261  0.012143  0.975279  0.941526  0.960734\n",
      "f1_t_end    0.957039  0.014717  0.973825  0.933693  0.957617\n",
      "Number of labeled data: 20\n",
      "Training Stage\n",
      "Test Stage\n",
      "                mean       std       max       min     micro\n",
      "iou_p       0.765633  0.021922  0.785267  0.736850  0.760189\n",
      "iou_qrs     0.866500  0.010927  0.885099  0.858954  0.864755\n",
      "iou_t       0.832087  0.019269  0.865621  0.817504   0.83224\n",
      "miou        0.821406  0.014650  0.844519  0.806310  0.819061\n",
      "acc         0.920805  0.006611  0.931926  0.915636   0.91993\n",
      "ave_f1      0.971657  0.006595  0.979093  0.963314  0.970318\n",
      "f1_p_on     0.948868  0.009399  0.961683  0.936375  0.944525\n",
      "f1_p_end    0.949414  0.009481  0.961683  0.936375  0.945055\n",
      "f1_qrs_on   0.994017  0.002710  0.996549  0.989944  0.994173\n",
      "f1_qrs_end  0.993925  0.002639  0.996319  0.990189  0.993629\n",
      "f1_t_on     0.972008  0.009039  0.983439  0.963362  0.972413\n",
      "f1_t_end    0.971711  0.010087  0.983949  0.962049  0.972112\n",
      "Number of labeled data: 50\n",
      "Training Stage\n",
      "Test Stage\n",
      "                mean       std       max       min     micro\n",
      "iou_p       0.805263  0.028937  0.829704  0.758316  0.798026\n",
      "iou_qrs     0.884478  0.008782  0.898883  0.875530  0.882892\n",
      "iou_t       0.851825  0.012456  0.870455  0.839296  0.851873\n",
      "miou        0.847188  0.014217  0.865556  0.826049  0.844264\n",
      "acc         0.932269  0.006026  0.941289  0.926154  0.931212\n",
      "ave_f1      0.979334  0.006688  0.986487  0.968928  0.977647\n",
      "f1_p_on     0.959804  0.014362  0.975030  0.938396  0.954309\n",
      "f1_p_end    0.959679  0.014738  0.975030  0.937481  0.954191\n",
      "f1_qrs_on   0.996384  0.001625  0.998886  0.994343  0.996459\n",
      "f1_qrs_end  0.996384  0.001625  0.998886  0.994343  0.996005\n",
      "f1_t_on     0.982386  0.007596  0.988553  0.972858  0.982964\n",
      "f1_t_end    0.981368  0.007378  0.988071  0.973388  0.981956\n",
      "Number of labeled data: 160\n",
      "Training Stage\n",
      "Fold 1/5: Train labeled: 1536, Train unlabeled: 23970, Val: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 80/80 [1:46:28<00:00, 79.85s/epoch, loss: 1.9787, val_loss: 0.7986, mask_ratio: 0.9504]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5: Train labeled: 1536, Train unlabeled: 23970, Val: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 80/80 [1:51:43<00:00, 83.79s/epoch, loss: 1.9766, val_loss: 0.8083, mask_ratio: 0.9493]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5: Train labeled: 1536, Train unlabeled: 23970, Val: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 80/80 [1:50:42<00:00, 83.03s/epoch, loss: 1.9823, val_loss: 0.7997, mask_ratio: 0.9504]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5: Train labeled: 1536, Train unlabeled: 23970, Val: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 80/80 [1:50:45<00:00, 83.06s/epoch, loss: 1.9872, val_loss: 0.7970, mask_ratio: 0.9359]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5: Train labeled: 1536, Train unlabeled: 23970, Val: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 80/80 [1:13:39<00:00, 55.24s/epoch, loss: 1.9758, val_loss: 0.8016, mask_ratio: 0.9524]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Stage\n",
      "                mean       std       max       min     micro\n",
      "iou_p       0.832853  0.024862  0.862002  0.794037  0.828636\n",
      "iou_qrs     0.895128  0.009440  0.910049  0.885964  0.893753\n",
      "iou_t       0.865026  0.018801  0.891470  0.848521  0.864982\n",
      "miou        0.864335  0.016270  0.887841  0.843700  0.862457\n",
      "acc         0.939555  0.006987  0.951018  0.933638  0.938851\n",
      "ave_f1      0.983367  0.006941  0.992250  0.973976  0.982482\n",
      "f1_p_on     0.971423  0.011564  0.985021  0.953525   0.96856\n",
      "f1_p_end    0.971610  0.011412  0.985021  0.953841  0.968737\n",
      "f1_qrs_on   0.994255  0.004040  0.997775  0.987492   0.99425\n",
      "f1_qrs_end  0.994299  0.004119  0.997775  0.987492  0.993842\n",
      "f1_t_on     0.984407  0.008605  0.994602  0.973659  0.984851\n",
      "f1_t_end    0.984207  0.008555  0.994602  0.973659  0.984649\n"
     ]
    }
   ],
   "source": [
    "th_delineation = 150\n",
    "gpu = 3\n",
    "aug = 2\n",
    "deep_supervision = 1\n",
    "torch.cuda.set_device(gpu)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Parameters\n",
    "model_save_path = f'./checkpoints/unet_a_ds{int(deep_supervision)}_UniMatch-V2.cross'\n",
    "metrics_save_path = f'./metrics/unet_a_ds{int(deep_supervision)}_UniMatch-V2.cross'\n",
    "records_list = [5,10,20,50,160]\n",
    "val_ratio = 0.2\n",
    "df_list = []\n",
    "\n",
    "for num_labeled in records_list:\n",
    "    ## Train\n",
    "    print(f\"Number of labeled data: {num_labeled}\")\n",
    "    print(\"Training Stage\")\n",
    "    for fold in range(5):\n",
    "        if os.path.exists(f\"{model_save_path}.num_labeled_{num_labeled}_fold_{fold}.final.pth\"):\n",
    "            continue\n",
    "        # Set random seed for reproducibility\n",
    "        seed = 42\n",
    "        cudnn.benchmark = False\n",
    "        cudnn.deterministic = True\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "        x_train_l, y_train_l, _, _, _ = raw_data_load_ludb(40, num_labeled, fold, crop=[1250, 3750])\n",
    "        x_train_u, y_train_u, _, _, _ = raw_data_load_rdb(400, 1999, fold, crop=[1250, 3750])\n",
    "\n",
    "        num_val =  np.round(x_train_l.shape[0] * val_ratio).astype(int)\n",
    "        x_val, y_val = x_train_l[:num_val], y_train_l[:num_val]\n",
    "        x_train_l, y_train_l = x_train_l[num_val:], y_train_l[num_val:]\n",
    "\n",
    "        print(f\"Fold {fold+1}/{5}: Train labeled: {x_train_l.shape[0]}, Train unlabeled: {x_train_u.shape[0]}, Val: {x_val.shape[0]}\")\n",
    "        \n",
    "        model = UNet1D_A(length=2500, base_channels=16, kernel_size=9, dropout='channels', droprate=.2, num_classes=2).to('cuda')\n",
    "        ini_ds = deep_supervision\n",
    "        ini_aug = 2\n",
    "        model_load_path = f\"./checkpoints/unet_a_ds{ini_ds}.num_labeled_{num_labeled}_aug_{ini_aug}.fold_{fold}.epoch_20.pth\"\n",
    "        model.load_state_dict(torch.load(model_load_path))\n",
    "        model_ema = deepcopy(model)\n",
    "        model_ema.eval()\n",
    "        for param in model_ema.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        trainset_u = SemiDataset(x_train_u, y_train_u, 'train_u')\n",
    "        trainset_l = SemiDataset(x_train_l, y_train_l, 'train_l')\n",
    "        valset = SemiDataset(x_val, y_val, 'val')\n",
    "\n",
    "        model_path = f\"{model_save_path}.num_labeled_{num_labeled}_fold_{fold}\"\n",
    "        train_Unimatch_V2(model, model_ema, trainset_u, trainset_l, valset, model_path, deep_supervision=deep_supervision)\n",
    "\n",
    "    ## Test\n",
    "    print(\"Test Stage\")\n",
    "    data = []\n",
    "    label = []\n",
    "    preds = []\n",
    "    seg_metrics_macro = []\n",
    "    deli_metrics_macro = []\n",
    "    \n",
    "    for fold in range(5):\n",
    "        model = UNet1D_A(length=2500, base_channels=16, kernel_size=9, dropout='channels', droprate=.2, num_classes=2).to(device)\n",
    "        model_load_path = f\"{model_save_path}.num_labeled_{num_labeled}_fold_{fold}.best.pth\"\n",
    "        model.load_state_dict(torch.load(model_load_path))\n",
    "\n",
    "        x_train, y_train, _, x_test, y_test = raw_data_load_ludb(40, 160, fold, crop=[0, 5000])\n",
    "        test_dataset = ECGDataset(x_test, y_test, transform=base_transforms())\n",
    "\n",
    "        pred = model_predict(model, model_load_path, test_dataset, device, multi_lead_correction=False)\n",
    "        \n",
    "        flag_ludb = np.load('./dataset/ludb/flag.npy')\n",
    "        index_shuffled_5fold = np.load('./dataset/ludb/ludb_index_shuffled_5fold_250113.npy')\n",
    "        index_shuffled = index_shuffled_5fold[:,fold]\n",
    "        index_shuffled_lead = []\n",
    "        for i in np.array(index_shuffled):\n",
    "            index_shuffled_lead.extend([k for k in range(12*i,12*i+12,1)])\n",
    "        num_test = 40\n",
    "        flag_test= flag_ludb[index_shuffled_lead[0:num_test*12]]\n",
    "        dataset = (x_test, y_test, np.zeros((x_test.shape[0],)), flag_test)\n",
    "        _, _, seg_metrics, deli_metrics = dataset_eval(dataset, pred, th_delineation=th_delineation, verbose=0)\n",
    "\n",
    "        data.append(x_test)\n",
    "        label.append(y_test)\n",
    "        preds.append(pred)\n",
    "        seg_metrics_macro.append(seg_metrics)\n",
    "        deli_metrics_macro.append(deli_metrics)\n",
    "\n",
    "    data = np.concatenate(data, axis=0)\n",
    "    label = np.concatenate(label, axis=0)\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "\n",
    "    def summarize_total_rows(dfs):\n",
    "        \"\"\"\n",
    "        Extracts 'Total' rows from DataFrames, calculates summary statistics,\n",
    "        and returns a new DataFrame.\n",
    "\n",
    "        Args:\n",
    "        dfs: A list of pandas DataFrames with identical structure.\n",
    "\n",
    "        Returns:\n",
    "            A pandas DataFrame containing the mean, std, max, and min\n",
    "            of each column of the 'Total' rows from all input DataFrames.\n",
    "        \"\"\"\n",
    "        total_rows = [df[df['type'] == 'Total'].iloc[0] for df in dfs]\n",
    "        total_df = pd.DataFrame(total_rows)\n",
    "\n",
    "        # Get original headers, and remove 'type'\n",
    "        original_headers = total_df.columns.tolist()\n",
    "        original_headers.remove('type')\n",
    "\n",
    "\n",
    "        # Calculate summary stats for each column (excluding type)\n",
    "        summary_data = {\n",
    "            'mean': total_df[original_headers].mean().to_list(),\n",
    "            'std': total_df[original_headers].std().to_list(),\n",
    "            'max': total_df[original_headers].max().to_list(),\n",
    "            'min': total_df[original_headers].min().to_list()\n",
    "        }\n",
    "        # Create the summary DataFrame\n",
    "        summary_df = pd.DataFrame(summary_data, index = original_headers)\n",
    "        return summary_df\n",
    "    \n",
    "    # Macro average metrics of 5 folds\n",
    "    seg_metrics_macro = summarize_total_rows(seg_metrics_macro)\n",
    "    deli_metrics_macro = summarize_total_rows(deli_metrics_macro) \n",
    "    filtered_df = deli_metrics_macro[deli_metrics_macro.index.str.contains('f1')]\n",
    "    merged_df_macro = pd.concat([seg_metrics_macro, filtered_df], axis = 0)                \n",
    "\n",
    "    # Micro average metrics of 5 folds\n",
    "    dataset = (data, label, np.zeros((data.shape[0],)), np.zeros((data.shape[0],)))\n",
    "    _, _, seg_metrics_micro, deli_metrics_micro = dataset_eval(dataset, preds, th_delineation=th_delineation, verbose=0)\n",
    "    # Filter df2 to include rows where column name contain 'f1'\n",
    "    filtered_df = deli_metrics_micro[['type'] + [col for col in deli_metrics_micro.columns if 'f1' in col]]\n",
    "    merged_df_micro = pd.merge(seg_metrics_micro, filtered_df, on='type', how='outer')\n",
    "    micro_row = merged_df_micro[merged_df_micro['type'] == 'Total'].iloc[0]\n",
    "    # Remove 'type' and convert to series\n",
    "    micro_row_values = micro_row.drop('type')\n",
    "    merged_df = copy.deepcopy(merged_df_macro)\n",
    "    merged_df['micro'] = micro_row_values\n",
    "    \n",
    "    df_list.append(merged_df)\n",
    "    # Final results\n",
    "    print(merged_df)\n",
    "\n",
    "\n",
    "# Save results\n",
    "# 拼接数据，将总标题作为列上方的“标题行”\n",
    "concat_frames = []\n",
    "for i, df in enumerate(df_list):\n",
    "    # 插入标题行\n",
    "    df_with_title = df.copy()\n",
    "    df_with_title.columns = pd.MultiIndex.from_tuples([(str(records_list[i]), col) for col in df.columns])\n",
    "    concat_frames.append(df_with_title)\n",
    "\n",
    "# 按列拼接，并保留行标题\n",
    "result = pd.concat(concat_frames, axis=1)\n",
    "\n",
    "# 写入 Excel\n",
    "result.to_excel(f\"{metrics_save_path}.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
